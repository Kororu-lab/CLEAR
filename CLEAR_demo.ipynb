{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAR System Demo Notebook\n",
    "\n",
    "## NAVER AiRS 기반 뉴스 추천 및 주식 영향 분석 시스템\n",
    "\n",
    "이 노트북은 NAVER의 AiRS 알고리즘을 기반으로 한 뉴스 추천 및 주식 영향 분석 시스템인 CLEAR의 기능을 시연합니다. 이 시스템은 뉴스 클러스터링, 추천, 그리고 주식 가격에 미치는 영향을 분석하는 기능을 포함합니다.\n",
    "\n",
    "This notebook demonstrates the functionality of CLEAR, a news recommendation and stock impact analysis system based on NAVER's AiRS algorithm. The system includes news clustering, recommendation, and analysis of impact on stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 시스템 개요 (System Overview)\n",
    "\n",
    "CLEAR 시스템은 다음과 같은 주요 구성 요소로 이루어져 있습니다:\n",
    "1. 데이터 처리 (뉴스 및 주식 데이터)\n",
    "2. 텍스트 전처리\n",
    "3. 뉴스 벡터화\n",
    "4. 뉴스 클러스터링\n",
    "5. 주식 영향 분석\n",
    "6. 뉴스 추천\n",
    "\n",
    "The CLEAR system consists of the following main components:\n",
    "1. Data processing (news and stock data)\n",
    "2. Text preprocessing\n",
    "3. News vectorization\n",
    "4. News clustering\n",
    "5. Stock impact analysis\n",
    "6. News recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 숨기기\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시스템 경로에 src 디렉토리 추가\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"CLEAR 시스템 데모 환경이 준비되었습니다.\")\n",
    "print(\"CLEAR system demo environment is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 (Data Loading)\n",
    "\n",
    "먼저 뉴스 데이터와 주식 데이터를 로드합니다. 이 데이터는 시스템의 기본 입력으로 사용됩니다.\n",
    "\n",
    "First, we load the news data and stock data. This data is used as the basic input for the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 뉴스 데이터 로드\n",
    "news_data_path = os.path.join(os.getcwd(), 'data', 'news', 'yna_005930_all.csv')\n",
    "news_df = pd.read_csv(news_data_path)\n",
    "\n",
    "# 주식 데이터 로드\n",
    "stock_data_path = os.path.join(os.getcwd(), 'data', 'stock', 'stockprice_005930.csv')\n",
    "stock_df = pd.read_csv(stock_data_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"뉴스 데이터 샘플 수: {len(news_df)}\")\n",
    "print(f\"주식 데이터 샘플 수: {len(stock_df)}\")\n",
    "\n",
    "# 뉴스 데이터 샘플 확인\n",
    "print(\"\\n뉴스 데이터 샘플:\")\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 주식 데이터 샘플 확인\n",
    "print(\"주식 데이터 샘플:\")\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "뉴스 데이터와 주식 데이터를 분석에 적합한 형태로 전처리합니다.\n",
    "\n",
    "We preprocess the news data and stock data into a form suitable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 날짜 형식 변환 함수\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        # 예: '20250101 18:56' -> '2025-01-01 18:56:00'\n",
    "        date_parts = date_str.split(' ')\n",
    "        date = date_parts[0]\n",
    "        time = date_parts[1] if len(date_parts) > 1 else '00:00'\n",
    "        \n",
    "        year = date[:4]\n",
    "        month = date[4:6]\n",
    "        day = date[6:8]\n",
    "        \n",
    "        return f\"{year}-{month}-{day} {time}:00\"\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 변환 오류: {e} - {date_str}\")\n",
    "        return None\n",
    "\n",
    "# 뉴스 데이터 전처리\n",
    "news_df['Date'] = news_df['Date'].apply(convert_date_format)\n",
    "news_df['datetime'] = pd.to_datetime(news_df['Date'])\n",
    "news_df = news_df.sort_values('datetime')\n",
    "\n",
    "# 주식 데이터 전처리\n",
    "stock_df['datetime'] = pd.to_datetime(stock_df['Date'] + ' ' + stock_df['Time'])\n",
    "stock_df = stock_df.sort_values('datetime')\n",
    "\n",
    "# 주식 가격 변동 계산\n",
    "stock_df['price_change'] = stock_df['End'] - stock_df['Start']\n",
    "stock_df['price_change_pct'] = (stock_df['price_change'] / stock_df['Start']) * 100\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(\"전처리된 뉴스 데이터:\")\n",
    "news_df[['Title', 'datetime', 'Press']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 전처리된 주식 데이터 확인\n",
    "print(\"전처리된 주식 데이터:\")\n",
    "stock_df[['datetime', 'Start', 'End', 'price_change', 'price_change_pct']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처리 (Text Preprocessing)\n",
    "\n",
    "뉴스 텍스트를 분석에 적합한 형태로 전처리합니다. 이 과정에는 불용어 제거, 토큰화 등이 포함됩니다.\n",
    "\n",
    "We preprocess the news text into a form suitable for analysis. This process includes removing stopwords, tokenization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 텍스트 전처리 모듈 임포트\n",
    "from src.data.text_preprocessor import TextPreprocessor\n",
    "\n",
    "# 텍스트 전처리기 초기화\n",
    "preprocessor = TextPreprocessor(\n",
    "    language='korean',\n",
    "    remove_stopwords=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_numbers=False,\n",
    "    use_mecab=True\n",
    ")\n",
    "\n",
    "# 샘플 텍스트 전처리\n",
    "sample_text = news_df['Title'].iloc[0] + \". \" + news_df['Body'].iloc[0][:200]\n",
    "processed_text = preprocessor.preprocess_text(sample_text)\n",
    "\n",
    "print(\"원본 텍스트:\")\n",
    "print(sample_text)\n",
    "print(\"\\n전처리된 텍스트:\")\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 뉴스 데이터 전체 전처리\n",
    "print(\"뉴스 데이터 전처리 중...\")\n",
    "\n",
    "# 제목 전처리\n",
    "news_df['processed_title'] = news_df['Title'].apply(preprocessor.preprocess_text)\n",
    "\n",
    "# 본문 전처리 (시간 절약을 위해 일부만 처리)\n",
    "news_df['processed_body'] = news_df['Body'].apply(\n",
    "    lambda x: preprocessor.preprocess_text(x[:500]) if isinstance(x, str) else \"\"\n",
    ")\n",
    "\n",
    "print(\"전처리 완료!\")\n",
    "\n",
    "# 전처리 결과 확인\n",
    "news_df[['Title', 'processed_title', 'processed_body']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 뉴스 벡터화 (News Vectorization)\n",
    "\n",
    "전처리된 뉴스 텍스트를 벡터로 변환합니다. 여러 벡터화 방법을 비교해 보겠습니다.\n",
    "\n",
    "We convert the preprocessed news text into vectors. Let's compare several vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 뉴스 벡터화 모듈 임포트\n",
    "from src.models.news_vectorizer import NewsVectorizer\n",
    "\n",
    "# 기본 벡터화 방법 (TF-IDF)\n",
    "tfidf_vectorizer = NewsVectorizer(\n",
    "    method='tfidf',\n",
    "    title_weight=0.7,\n",
    "    content_weight=0.3,\n",
    "    max_features=100\n",
    ")\n",
    "\n",
    "# Word2Vec 벡터화 방법\n",
    "w2v_vectorizer = NewsVectorizer(\n",
    "    method='word2vec',\n",
    "    title_weight=0.7,\n",
    "    content_weight=0.3,\n",
    "    vector_size=100\n",
    ")\n",
    "\n",
    "# 벡터화 수행\n",
    "print(\"TF-IDF 벡터화 수행 중...\")\n",
    "tfidf_vectors = tfidf_vectorizer.vectorize_dataframe(\n",
    "    news_df,\n",
    "    title_col='processed_title',\n",
    "    content_col='processed_body'\n",
    ")\n",
    "\n",
    "print(\"Word2Vec 벡터화 수행 중...\")\n",
    "w2v_vectors = w2v_vectorizer.vectorize_dataframe(\n",
    "    news_df,\n",
    "    title_col='processed_title',\n",
    "    content_col='processed_body'\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF 벡터 형태: {tfidf_vectors.shape}\")\n",
    "print(f\"Word2Vec 벡터 형태: {w2v_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 고급 임베딩 방법 (Advanced Embedding Methods)\n",
    "\n",
    "이제 한국어에 특화된 고급 임베딩 방법을 사용하여 뉴스 텍스트를 벡터화해 보겠습니다.\n",
    "\n",
    "Now, let's vectorize news text using advanced embedding methods specialized for Korean language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 한국어 임베딩 향상 모듈 임포트\n",
    "from src.models.korean_embedding_enhancer import KoreanEmbeddingEnhancer\n",
    "\n",
    "# 한국어 임베딩 향상기 초기화\n",
    "embedding_enhancer = KoreanEmbeddingEnhancer(\n",
    "    use_kobert=True,\n",
    "    use_klue_roberta=True,\n",
    "    use_kosimcse=True,\n",
    "    use_bge_korean=True,\n",
    "    cache_embeddings=True\n",
    ")\n",
    "\n",
    "# 사용 가능한 모델 확인\n",
    "available_models = embedding_enhancer.get_available_models()\n",
    "print(f\"사용 가능한 모델: {available_models}\")\n",
    "\n",
    "# 샘플 텍스트에 대한 임베딩 생성\n",
    "sample_texts = news_df['Title'].head(3).tolist()\n",
    "\n",
    "for model in available_models[:2]:  # 처음 두 모델만 시연\n",
    "    print(f\"\\n{model} 모델을 사용한 임베딩:\")\n",
    "    for text in sample_texts:\n",
    "        embedding = embedding_enhancer.get_embedding(text, model)\n",
    "        print(f\"텍스트: {text[:30]}...\")\n",
    "        print(f\"임베딩 차원: {embedding.shape}\")\n",
    "        print(f\"임베딩 샘플: {embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 앙상블 임베딩 생성\n",
    "print(\"앙상블 임베딩 생성 중...\")\n",
    "ensemble_embeddings = embedding_enhancer.get_ensemble_embeddings(\n",
    "    sample_texts,\n",
    "    models=available_models[:2],  # 처음 두 모델만 사용\n",
    "    weights=[0.6, 0.4]  # 가중치 설정\n",
    ")\n",
    "\n",
    "print(f\"앙상블 임베딩 형태: {ensemble_embeddings.shape}\")\n",
    "print(f\"앙상블 임베딩 샘플:\\n{ensemble_embeddings[0][:5]}...\")\n",
    "\n",
    "# 임베딩 간 유사도 계산\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(ensemble_embeddings)\n",
    "print(\"\\n임베딩 간 유사도 행렬:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 뉴스 클러스터링 (News Clustering)\n",
    "\n",
    "벡터화된 뉴스를 클러스터링하여 유사한 뉴스 그룹을 식별합니다.\n",
    "\n",
    "We cluster the vectorized news to identify groups of similar news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 뉴스 클러스터링 모듈 임포트\n",
    "from src.models.news_clustering import NewsClustering\n",
    "\n",
    "# 클러스터링 모델 초기화\n",
    "clustering_model = NewsClustering(\n",
    "    method='kmeans',\n",
    "    n_clusters=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 클러스터링 수행\n",
    "print(\"TF-IDF 벡터를 사용한 클러스터링 수행 중...\")\n",
    "tfidf_clusters = clustering_model.cluster(tfidf_vectors)\n",
    "\n",
    "print(\"Word2Vec 벡터를 사용한 클러스터링 수행 중...\")\n",
    "w2v_clusters = clustering_model.cluster(w2v_vectors)\n",
    "\n",
    "# 클러스터링 결과 저장\n",
    "news_df['tfidf_cluster'] = tfidf_clusters\n",
    "news_df['w2v_cluster'] = w2v_clusters\n",
    "\n",
    "# 클러스터링 결과 확인\n",
    "print(\"\\nTF-IDF 클러스터 분포:\")\n",
    "print(news_df['tfidf_cluster'].value_counts())\n",
    "\n",
    "print(\"\\nWord2Vec 클러스터 분포:\")\n",
    "print(news_df['w2v_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 클러스터 시각화\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA를 사용하여 2차원으로 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca = pca.fit_transform(tfidf_vectors)\n",
    "\n",
    "# 클러스터 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster in range(clustering_model.n_clusters):\n",
    "    mask = tfidf_clusters == cluster\n",
    "    plt.scatter(tfidf_pca[mask, 0], tfidf_pca[mask, 1], label=f'Cluster {cluster}', alpha=0.7)\n",
    "\n",
    "plt.title('TF-IDF 벡터를 사용한 뉴스 클러스터링 결과')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 고급 클러스터링 방법 (Advanced Clustering Methods)\n",
    "\n",
    "이제 고급 임베딩을 사용하여 뉴스 클러스터링을 수행해 보겠습니다.\n",
    "\n",
    "Now, let's perform news clustering using advanced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 샘플 데이터에 대한 임베딩 생성\n",
    "sample_df = news_df.head(50)  # 처리 시간 단축을 위해 일부만 사용\n",
    "\n",
    "# 데이터프레임 벡터화\n",
    "print(\"KoBERT를 사용한 벡터화 수행 중...\")\n",
    "kobert_vectors = embedding_enhancer.vectorize_dataframe(\n",
    "    sample_df,\n",
    "    text_col='Title',\n",
    "    content_col='Body',\n",
    "    use_content=True,\n",
    "    model='kobert',\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# 앙상블 벡터화\n",
    "print(\"앙상블 벡터화 수행 중...\")\n",
    "ensemble_vectors = embedding_enhancer.vectorize_dataframe_ensemble(\n",
    "    sample_df,\n",
    "    text_col='Title',\n",
    "    content_col='Body',\n",
    "    use_content=True,\n",
    "    models=available_models[:2],\n",
    "    weights=[0.6, 0.4],\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "print(f\"KoBERT 벡터 형태: {kobert_vectors.shape}\")\n",
    "print(f\"앙상블 벡터 형태: {ensemble_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 고급 클러스터링 수행\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# 계층적 클러스터링 모델 초기화\n",
    "hierarchical_clustering = AgglomerativeClustering(\n",
    "    n_clusters=5,\n",
    "    affinity='cosine',\n",
    "    linkage='average'\n",
    ")\n",
    "\n",
    "# 클러스터링 수행\n",
    "kobert_clusters = hierarchical_clustering.fit_predict(kobert_vectors)\n",
    "ensemble_clusters = hierarchical_clustering.fit_predict(ensemble_vectors)\n",
    "\n",
    "# 클러스터링 결과 저장\n",
    "sample_df['kobert_cluster'] = kobert_clusters\n",
    "sample_df['ensemble_cluster'] = ensemble_clusters\n",
    "\n",
    "# 클러스터링 결과 확인\n",
    "print(\"\\nKoBERT 클러스터 분포:\")\n",
    "print(sample_df['kobert_cluster'].value_counts())\n",
    "\n",
    "print(\"\\n앙상블 클러스터 분포:\")\n",
    "print(sample_df['ensemble_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 클러스터 시각화\n",
    "# PCA를 사용하여 2차원으로 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "ensemble_pca = pca.fit_transform(ensemble_vectors)\n",
    "\n",
    "# 클러스터 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster in range(5):\n",
    "    mask = ensemble_clusters == cluster\n",
    "    plt.scatter(ensemble_pca[mask, 0], ensemble_pca[mask, 1], label=f'Cluster {cluster}', alpha=0.7)\n",
    "\n",
    "plt.title('앙상블 임베딩을 사용한 뉴스 클러스터링 결과')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 주식 영향 분석 (Stock Impact Analysis)\n",
    "\n",
    "뉴스가 주식 가격에 미치는 영향을 분석합니다.\n",
    "\n",
    "We analyze the impact of news on stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 주식 영향 분석 모듈 임포트\n",
    "from src.models.stock_impact_analyzer import StockImpactAnalyzer\n",
    "\n",
    "# 주식 영향 분석기 초기화\n",
    "impact_analyzer = StockImpactAnalyzer(\n",
    "    lookback_window=3,  # 3일 전부터의 데이터 사용\n",
    "    lookahead_window=3,  # 3일 후까지의 데이터 사용\n",
    "    impact_threshold=0.5  # 영향력 임계값\n",
    ")\n",
    "\n",
    "# 주식 영향 분석 수행\n",
    "print(\"주식 영향 분석 수행 중...\")\n",
    "impact_scores = impact_analyzer.analyze_impact(\n",
    "    news_df,\n",
    "    stock_df,\n",
    "    news_date_col='datetime',\n",
    "    stock_date_col='datetime',\n",
    "    price_col='End'\n",
    ")\n",
    "\n",
    "# 영향 점수 저장\n",
    "news_df['impact_score'] = impact_scores\n",
    "\n",
    "# 영향 점수 분포 확인\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(news_df['impact_score'].dropna(), bins=20, alpha=0.7)\n",
    "plt.title('뉴스 영향 점수 분포')\n",
    "plt.xlabel('영향 점수')\n",
    "plt.ylabel('뉴스 수')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 영향력이 큰 뉴스 확인\n",
    "high_impact_news = news_df.sort_values('impact_score', ascending=False).head(5)\n",
    "print(\"\\n영향력이 큰 뉴스 Top 5:\")\n",
    "for idx, row in high_impact_news.iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"날짜: {row['Date']}\")\n",
    "    print(f\"영향 점수: {row['impact_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 고급 주식 영향 분석 (Advanced Stock Impact Analysis)\n",
    "\n",
    "이제 다양한 임베딩 모델을 사용한 고급 주식 영향 분석 방법을 시연합니다.\n",
    "\n",
    "Now, we demonstrate advanced stock impact analysis methods using various embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 고급 점수 계산 방법 모듈 임포트\n",
    "from src.models.advanced_scoring_methods import AdvancedScoringMethods\n",
    "\n",
    "# 고급 점수 계산 방법 초기화\n",
    "scoring_methods = AdvancedScoringMethods(\n",
    "    use_kosimcse=True,\n",
    "    use_bge_korean=True,\n",
    "    cache_embeddings=True\n",
    ")\n",
    "\n",
    "# 감성 점수 계산 (간단한 방법으로 대체)\n",
    "def calculate_simple_sentiment(text):\n",
    "    positive_words = ['상승', '급등', '호조', '성장', '개선', '흑자', '최대', '신기록']\n",
    "    negative_words = ['하락', '급락', '부진', '감소', '악화', '적자', '손실', '하향']\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "        \n",
    "    pos_count = sum(1 for word in positive_words if word in text)\n",
    "    neg_count = sum(1 for word in negative_words if word in text)\n",
    "    \n",
    "    if pos_count + neg_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (pos_count - neg_count) / (pos_count + neg_count)\n",
    "\n",
    "# 샘플 데이터에 감성 점수 추가\n",
    "sample_df['sentiment_score'] = sample_df['Title'].apply(calculate_simple_sentiment)\n",
    "\n",
    "# 주가 변동 계산 (간단한 방법으로 대체)\n",
    "sample_df['price_change_pct_1d'] = 0.5  # 예시 값\n",
    "\n",
    "# 앙상블 영향 점수 계산\n",
    "print(\"앙상블 영향 점수 계산 중...\")\n",
    "impact_results = scoring_methods.calculate_ensemble_impact_scores(\n",
    "    sample_df,\n",
    "    text_col='Title',\n",
    "    sentiment_col='sentiment_score',\n",
    "    price_change_col='price_change_pct_1d',\n",
    "    models=['kosimcse', 'bge_korean'],\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\n앙상블 영향 점수 결과:\")\n",
    "impact_results[['Title', 'sentiment_score', 'impact_kosimcse', 'impact_bge_korean', 'impact_ensemble']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 모델 비교 시각화\n",
    "scoring_methods.visualize_model_comparison(\n",
    "    impact_results,\n",
    "    models=['kosimcse', 'bge_korean']\n",
    ")\n",
    "\n",
    "# 앙상블 가중치 시각화\n",
    "scoring_methods.visualize_ensemble_weights(\n",
    "    weights=[0.6, 0.4],\n",
    "    models=['kosimcse', 'bge_korean']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 한국어 텍스트 분석 (Korean Text Analysis)\n",
    "\n",
    "한국어 금융 텍스트에 대한 고급 감성 분석을 수행합니다.\n",
    "\n",
    "We perform advanced sentiment analysis on Korean financial text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 한국어 텍스트 분석 모듈 임포트\n",
    "from src.models.korean_text_analyzer import KoreanFinancialTextAnalyzer\n",
    "\n",
    "# 한국어 텍스트 분석기 초기화\n",
    "text_analyzer = KoreanFinancialTextAnalyzer(\n",
    "    use_finbert=True,\n",
    "    use_advanced_embeddings=True,\n",
    "    cache_results=True\n",
    ")\n",
    "\n",
    "# 샘플 텍스트 분석\n",
    "sample_texts = [\n",
    "    \"삼성전자 주가 상승, 실적 개선 기대감에 투자자들 관심 집중\",\n",
    "    \"삼성전자 주가 하락, 실적 부진 우려에 투자자들 매도세 확대\",\n",
    "    \"삼성전자, 신제품 출시 계획 발표. 시장 반응은 중립적\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    result = text_analyzer.analyze_text(text)\n",
    "    print(f\"텍스트: {text}\")\n",
    "    print(f\"감성 점수: {result['sentiment_score']:.4f} ({result['sentiment_label']})\")\n",
    "    print(f\"신뢰도: {result['confidence']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 배치 분석\n",
    "batch_results = text_analyzer.analyze_batch(sample_texts)\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(batch_results)), [r['sentiment_score'] for r in batch_results], alpha=0.7)\n",
    "plt.xticks(range(len(batch_results)), [f\"Text {i+1}\" for i in range(len(batch_results))])\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.title('텍스트 감성 점수')\n",
    "plt.ylabel('감성 점수 (-1 ~ 1)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 뉴스 추천 (News Recommendation)\n",
    "\n",
    "사용자에게 관련성 높은 뉴스를 추천합니다.\n",
    "\n",
    "We recommend relevant news to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 뉴스 추천 모듈 임포트\n",
    "from src.models.news_recommender import NewsRecommender\n",
    "\n",
    "# 뉴스 추천기 초기화\n",
    "recommender = NewsRecommender(\n",
    "    cf_weight=0.3,  # 협업 필터링 가중치\n",
    "    cbf_weight=0.3,  # 콘텐츠 기반 필터링 가중치\n",
    "    si_weight=0.2,   # 소셜 영향력 가중치\n",
    "    latest_weight=0.1, # 최신성 가중치\n",
    "    stock_impact_weight=0.1 # 주식 영향력 가중치\n",
    ")\n",
    "\n",
    "# 추천 수행\n",
    "print(\"뉴스 추천 수행 중...\")\n",
    "recommended_news = recommender.recommend(\n",
    "    news_df,\n",
    "    user_id=None,  # 비개인화 추천\n",
    "    vectors=tfidf_vectors,\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "# 추천 결과 확인\n",
    "print(\"\\n추천 뉴스:\")\n",
    "for idx, row in recommended_news.iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"날짜: {row['Date']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 고급 뉴스 추천 (Advanced News Recommendation)\n",
    "\n",
    "이제 고급 임베딩과 다양한 점수 계산 방법을 사용한 뉴스 추천을 시연합니다.\n",
    "\n",
    "Now, we demonstrate news recommendation using advanced embeddings and various scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 고급 임베딩을 사용한 추천\n",
    "# 샘플 데이터에 대한 앙상블 임베딩 사용\n",
    "print(\"고급 임베딩을 사용한 추천 수행 중...\")\n",
    "advanced_recommended_news = recommender.recommend(\n",
    "    sample_df,\n",
    "    user_id=None,  # 비개인화 추천\n",
    "    vectors=ensemble_vectors,\n",
    "    top_n=3,\n",
    "    stock_impact_col='impact_ensemble'  # 앙상블 영향 점수 사용\n",
    ")\n",
    "\n",
    "# 추천 결과 확인\n",
    "print(\"\\n고급 임베딩을 사용한 추천 뉴스:\")\n",
    "for idx, row in advanced_recommended_news.iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"날짜: {row['datetime']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 다양한 가중치 조합 실험\n",
    "weight_combinations = [\n",
    "    {'cf': 0.2, 'cbf': 0.2, 'si': 0.2, 'latest': 0.2, 'stock_impact': 0.2},  # 균등 가중치\n",
    "    {'cf': 0.1, 'cbf': 0.1, 'si': 0.1, 'latest': 0.1, 'stock_impact': 0.6},  # 주식 영향력 중심\n",
    "    {'cf': 0.1, 'cbf': 0.6, 'si': 0.1, 'latest': 0.1, 'stock_impact': 0.1},  # 콘텐츠 중심\n",
    "    {'cf': 0.1, 'cbf': 0.1, 'si': 0.1, 'latest': 0.6, 'stock_impact': 0.1}   # 최신성 중심\n",
    "]\n",
    "\n",
    "# 각 가중치 조합에 대한 추천 결과 비교\n",
    "for i, weights in enumerate(weight_combinations):\n",
    "    # 추천기 초기화\n",
    "    custom_recommender = NewsRecommender(\n",
    "        cf_weight=weights['cf'],\n",
    "        cbf_weight=weights['cbf'],\n",
    "        si_weight=weights['si'],\n",
    "        latest_weight=weights['latest'],\n",
    "        stock_impact_weight=weights['stock_impact']\n",
    "    )\n",
    "    \n",
    "    # 추천 수행\n",
    "    recommended = custom_recommender.recommend(\n",
    "        sample_df,\n",
    "        user_id=None,\n",
    "        vectors=ensemble_vectors,\n",
    "        top_n=1,\n",
    "        stock_impact_col='impact_ensemble'\n",
    "    )\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"\\n가중치 조합 {i+1}:\")\n",
    "    print(f\"CF: {weights['cf']}, CBF: {weights['cbf']}, SI: {weights['si']}, Latest: {weights['latest']}, Stock Impact: {weights['stock_impact']}\")\n",
    "    print(f\"추천 뉴스: {recommended['Title'].iloc[0]}\")\n",
    "    print(f\"추천 점수: {recommended['recommendation_score'].iloc[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 실험적 모델 (Experimental Models)\n",
    "\n",
    "다양한 실험적 모델과 기법을 시연합니다.\n",
    "\n",
    "We demonstrate various experimental models and techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 다양한 임베딩 모델 비교\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 샘플 텍스트\n",
    "comparison_texts = [\n",
    "    \"삼성전자 신제품 출시, 시장 반응 긍정적\",\n",
    "    \"삼성전자 신형 스마트폰 공개, 소비자들 호평\",\n",
    "    \"애플 아이폰 신모델 발표, 삼성전자 경쟁 심화\",\n",
    "    \"반도체 시장 침체, 삼성전자 실적 우려\"\n",
    "]\n",
    "\n",
    "# 각 모델별 임베딩 생성\n",
    "model_embeddings = {}\n",
    "available_models = embedding_enhancer.get_available_models()\n",
    "\n",
    "for model in available_models[:3]:  # 처음 세 모델만 비교\n",
    "    print(f\"{model} 모델 임베딩 생성 중...\")\n",
    "    embeddings = [embedding_enhancer.get_embedding(text, model) for text in comparison_texts]\n",
    "    model_embeddings[model] = np.array(embeddings)\n",
    "\n",
    "# 앙상블 임베딩 생성\n",
    "print(\"앙상블 임베딩 생성 중...\")\n",
    "ensemble_embeddings = embedding_enhancer.get_ensemble_embeddings(\n",
    "    comparison_texts,\n",
    "    models=available_models[:3],\n",
    "    weights=[0.4, 0.3, 0.3]\n",
    ")\n",
    "model_embeddings['ensemble'] = ensemble_embeddings\n",
    "\n",
    "# 각 모델별 유사도 행렬 계산 및 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model, embeddings) in enumerate(model_embeddings.items()):\n",
    "    # 유사도 행렬 계산\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # 히트맵 시각화\n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, ax=axes[i])\n",
    "    axes[i].set_title(f'{model} 모델 유사도 행렬')\n",
    "    axes[i].set_xticklabels([f'Text {i+1}' for i in range(len(comparison_texts))], rotation=45)\n",
    "    axes[i].set_yticklabels([f'Text {i+1}' for i in range(len(comparison_texts))], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 다양한 클러스터링 알고리즘 비교\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 임베딩 데이터 준비 (앙상블 임베딩 사용)\n",
    "X = ensemble_embeddings\n",
    "\n",
    "# 클러스터링 알고리즘 정의\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=2),\n",
    "    'SpectralClustering': SpectralClustering(n_clusters=3, random_state=42)\n",
    "}\n",
    "\n",
    "# 각 알고리즘 실행 및 결과 비교\n",
    "results = {}\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"{name} 알고리즘 실행 중...\")\n",
    "    labels = algorithm.fit_predict(X)\n",
    "    results[name] = labels\n",
    "    \n",
    "    # 실루엣 점수 계산 (DBSCAN은 클러스터 수가 가변적이므로 제외)\n",
    "    if name != 'DBSCAN':\n",
    "        score = silhouette_score(X, labels)\n",
    "        print(f\"{name} 실루엣 점수: {score:.4f}\")\n",
    "    \n",
    "    # 클러스터 분포 출력\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"클러스터 수: {len(unique_labels)}\")\n",
    "    for label in unique_labels:\n",
    "        print(f\"클러스터 {label}: {np.sum(labels == label)}개 샘플\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 시스템 통합 (System Integration)\n",
    "\n",
    "전체 시스템을 통합하여 완전한 파이프라인을 시연합니다.\n",
    "\n",
    "We integrate the entire system to demonstrate a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 전체 시스템 통합 파이프라인\n",
    "def run_clear_pipeline(news_data, stock_data, use_advanced_models=False):\n",
    "    print(\"CLEAR 시스템 파이프라인 실행 중...\")\n",
    "    \n",
    "    # 1. 데이터 전처리\n",
    "    print(\"1. 데이터 전처리 중...\")\n",
    "    # 날짜 형식 변환\n",
    "    news_data['datetime'] = pd.to_datetime(news_data['Date'].apply(convert_date_format))\n",
    "    news_data = news_data.sort_values('datetime')\n",
    "    \n",
    "    stock_data['datetime'] = pd.to_datetime(stock_data['Date'] + ' ' + stock_data['Time'])\n",
    "    stock_data = stock_data.sort_values('datetime')\n",
    "    \n",
    "    # 주식 가격 변동 계산\n",
    "    stock_data['price_change'] = stock_data['End'] - stock_data['Start']\n",
    "    stock_data['price_change_pct'] = (stock_data['price_change'] / stock_data['Start']) * 100\n",
    "    \n",
    "    # 2. 텍스트 전처리\n",
    "    print(\"2. 텍스트 전처리 중...\")\n",
    "    preprocessor = TextPreprocessor(language='korean', remove_stopwords=True)\n",
    "    news_data['processed_title'] = news_data['Title'].apply(preprocessor.preprocess_text)\n",
    "    news_data['processed_body'] = news_data['Body'].apply(\n",
    "        lambda x: preprocessor.preprocess_text(x[:500]) if isinstance(x, str) else \"\"\n",
    "    )\n",
    "    \n",
    "    # 3. 뉴스 벡터화\n",
    "    print(\"3. 뉴스 벡터화 중...\")\n",
    "    if use_advanced_models:\n",
    "        # 고급 임베딩 사용\n",
    "        embedding_enhancer = KoreanEmbeddingEnhancer(\n",
    "            use_kobert=True,\n",
    "            use_klue_roberta=True\n",
    "        )\n",
    "        vectors = embedding_enhancer.vectorize_dataframe_ensemble(\n",
    "            news_data.head(50),  # 시연을 위해 일부만 사용\n",
    "            text_col='Title',\n",
    "            content_col='Body',\n",
    "            use_content=True,\n",
    "            models=['kobert', 'klue_roberta'],\n",
    "            weights=[0.6, 0.4],\n",
    "            batch_size=10\n",
    "        )\n",
    "        news_subset = news_data.head(50)\n",
    "    else:\n",
    "        # 기본 벡터화 방법 사용\n",
    "        vectorizer = NewsVectorizer(method='tfidf', title_weight=0.7, content_weight=0.3)\n",
    "        vectors = vectorizer.vectorize_dataframe(\n",
    "            news_data,\n",
    "            title_col='processed_title',\n",
    "            content_col='processed_body'\n",
    "        )\n",
    "        news_subset = news_data\n",
    "    \n",
    "    # 4. 뉴스 클러스터링\n",
    "    print(\"4. 뉴스 클러스터링 중...\")\n",
    "    clustering_model = NewsClustering(method='kmeans', n_clusters=5)\n",
    "    clusters = clustering_model.cluster(vectors)\n",
    "    news_subset['cluster'] = clusters\n",
    "    \n",
    "    # 5. 주식 영향 분석\n",
    "    print(\"5. 주식 영향 분석 중...\")\n",
    "    if use_advanced_models:\n",
    "        # 고급 점수 계산 방법 사용\n",
    "        # 감성 점수 계산 (간단한 방법으로 대체)\n",
    "        news_subset['sentiment_score'] = news_subset['Title'].apply(calculate_simple_sentiment)\n",
    "        news_subset['price_change_pct_1d'] = 0.5  # 예시 값\n",
    "        \n",
    "        scoring_methods = AdvancedScoringMethods(use_kosimcse=True)\n",
    "        impact_results = scoring_methods.calculate_ensemble_impact_scores(\n",
    "            news_subset,\n",
    "            text_col='Title',\n",
    "            sentiment_col='sentiment_score',\n",
    "            price_change_col='price_change_pct_1d'\n",
    "        )\n",
    "        news_subset = impact_results\n",
    "        impact_col = 'impact_ensemble'\n",
    "    else:\n",
    "        # 기본 영향 분석 방법 사용\n",
    "        impact_analyzer = StockImpactAnalyzer(lookback_window=3, lookahead_window=3)\n",
    "        impact_scores = impact_analyzer.analyze_impact(\n",
    "            news_subset,\n",
    "            stock_data,\n",
    "            news_date_col='datetime',\n",
    "            stock_date_col='datetime',\n",
    "            price_col='End'\n",
    "        )\n",
    "        news_subset['impact_score'] = impact_scores\n",
    "        impact_col = 'impact_score'\n",
    "    \n",
    "    # 6. 뉴스 추천\n",
    "    print(\"6. 뉴스 추천 중...\")\n",
    "    recommender = NewsRecommender(\n",
    "        cf_weight=0.2,\n",
    "        cbf_weight=0.3,\n",
    "        si_weight=0.2,\n",
    "        latest_weight=0.1,\n",
    "        stock_impact_weight=0.2\n",
    "    )\n",
    "    recommended_news = recommender.recommend(\n",
    "        news_subset,\n",
    "        user_id=None,  # 비개인화 추천\n",
    "        vectors=vectors,\n",
    "        top_n=5,\n",
    "        stock_impact_col=impact_col\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCLEAR 시스템 파이프라인 실행 완료!\")\n",
    "    return recommended_news\n",
    "\n",
    "# 기본 모델로 파이프라인 실행\n",
    "print(\"기본 모델로 파이프라인 실행:\")\n",
    "basic_recommendations = run_clear_pipeline(news_df.copy(), stock_df.copy(), use_advanced_models=False)\n",
    "\n",
    "print(\"\\n기본 모델 추천 결과:\")\n",
    "for idx, row in basic_recommendations.head(3).iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 고급 모델로 파이프라인 실행\n",
    "print(\"고급 모델로 파이프라인 실행:\")\n",
    "advanced_recommendations = run_clear_pipeline(news_df.copy(), stock_df.copy(), use_advanced_models=True)\n",
    "\n",
    "print(\"\\n고급 모델 추천 결과:\")\n",
    "for idx, row in advanced_recommendations.head(3).iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론 (Conclusion)\n",
    "\n",
    "이 노트북에서는 NAVER의 AiRS 알고리즘을 기반으로 한 뉴스 추천 및 주식 영향 분석 시스템인 CLEAR의 기능을 시연했습니다. 시스템은 다음과 같은 주요 구성 요소로 이루어져 있습니다:\n",
    "\n",
    "1. 데이터 처리 (뉴스 및 주식 데이터)\n",
    "2. 텍스트 전처리\n",
    "3. 뉴스 벡터화 (기본 및 고급 임베딩 방법)\n",
    "4. 뉴스 클러스터링\n",
    "5. 주식 영향 분석 (기본 및 고급 점수 계산 방법)\n",
    "6. 뉴스 추천\n",
    "\n",
    "또한 다양한 한국어 임베딩 모델과 점수 계산 방법을 비교하고, 이를 통합하여 완전한 파이프라인을 구현했습니다.\n",
    "\n",
    "In this notebook, we demonstrated the functionality of CLEAR, a news recommendation and stock impact analysis system based on NAVER's AiRS algorithm. The system consists of the following main components:\n",
    "\n",
    "1. Data processing (news and stock data)\n",
    "2. Text preprocessing\n",
    "3. News vectorization (basic and advanced embedding methods)\n",
    "4. News clustering\n",
    "5. Stock impact analysis (basic and advanced scoring methods)\n",
    "6. News recommendation\n",
    "\n",
    "We also compared various Korean embedding models and scoring methods, and integrated them to implement a complete pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
