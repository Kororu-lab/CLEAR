{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAR: 클러스터링 및 주가 영향 기반 뉴스 추천 시스템\n",
    "\n",
    "## 시스템 데모 노트북\n",
    "\n",
    "이 노트북은 CLEAR(Clustering and Stock Impact-based News Recommendation System) 시스템의 각 모듈을 단계별로 시연합니다. 네이버의 AiRs 아키텍처를 기반으로 하되, 개인화 대신 주가 영향에 중점을 둔 뉴스 추천 시스템입니다.\n",
    "\n",
    "### 목차\n",
    "1. 환경 설정 및 데이터 로드\n",
    "2. 텍스트 전처리\n",
    "3. 뉴스 벡터화\n",
    "4. 뉴스 클러스터링\n",
    "5. 주가 영향 분석\n",
    "6. 뉴스 추천\n",
    "7. 결과 시각화 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 데이터 로드\n",
    "\n",
    "먼저 필요한 라이브러리를 가져오고 시스템 구성 요소를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# 경로 설정\n",
    "sys.path.append('../src')\n",
    "\n",
    "# 시각화 스타일 설정\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAR 모듈 가져오기\n",
    "\n",
    "이제 CLEAR 시스템의 각 모듈을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAR 모듈 가져오기\n",
    "from src.data.text_preprocessor import TextPreprocessor\n",
    "from src.models.news_vectorizer import NewsVectorizer\n",
    "from src.models.news_clustering import NewsClustering\n",
    "from src.models.stock_impact_analyzer import StockImpactAnalyzer\n",
    "from src.models.news_recommender import NewsRecommender\n",
    "from src.evaluation import CLEAREvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드\n",
    "\n",
    "제공된 뉴스 및 주가 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "NEWS_DATA_PATH = 'data/news/yna_005930_2025.csv'\n",
    "STOCK_DATA_PATH = 'data/stock/stockprice_005930.csv'\n",
    "\n",
    "# 뉴스 데이터 로드\n",
    "news_df = pd.read_csv(NEWS_DATA_PATH)\n",
    "print(f\"로드된 뉴스 기사: {len(news_df)}개\")\n",
    "print(\"뉴스 데이터 구조:\")\n",
    "print(news_df.columns)\n",
    "print(\"\\n뉴스 데이터 샘플:\")\n",
    "display(news_df.head(3))\n",
    "\n",
    "# 주가 데이터 로드\n",
    "stock_df = pd.read_csv(STOCK_DATA_PATH)\n",
    "print(f\"\\n로드된 주가 데이터: {len(stock_df)}개 행\")\n",
    "print(\"주가 데이터 구조:\")\n",
    "print(stock_df.columns)\n",
    "print(\"\\n주가 데이터 샘플:\")\n",
    "display(stock_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 및 준비\n",
    "\n",
    "날짜 형식을 변환하고 데이터를 분석에 적합한 형태로 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 날짜 처리\n",
    "def process_news_date(date_str):\n",
    "    # 날짜 형식: \"20250101 18:56\"\n",
    "    date_parts = date_str.split()\n",
    "    date = date_parts[0]\n",
    "    time = date_parts[1] if len(date_parts) > 1 else \"00:00\"\n",
    "    \n",
    "    year = int(date[:4])\n",
    "    month = int(date[4:6])\n",
    "    day = int(date[6:8])\n",
    "    \n",
    "    hour, minute = map(int, time.split(':'))\n",
    "    \n",
    "    return datetime(year, month, day, hour, minute)\n",
    "\n",
    "# 주가 데이터 날짜 처리\n",
    "def process_stock_date(date_str, time_str):\n",
    "    # 날짜 형식: \"20250305\", 시간 형식: \"09:00\"\n",
    "    year = int(date_str[:4])\n",
    "    month = int(date_str[4:6])\n",
    "    day = int(date_str[6:8])\n",
    "    \n",
    "    hour, minute = map(int, time_str.split(':'))\n",
    "    \n",
    "    return datetime(year, month, day, hour, minute)\n",
    "\n",
    "# 뉴스 데이터 날짜 변환\n",
    "news_df['datetime'] = news_df['Date'].apply(process_news_date)\n",
    "news_df['date'] = news_df['datetime'].dt.date\n",
    "\n",
    "# 주가 데이터 날짜 변환\n",
    "stock_df['datetime'] = stock_df.apply(lambda row: process_stock_date(str(row['Date']), row['Time']), axis=1)\n",
    "stock_df['date'] = stock_df['datetime'].dt.date\n",
    "\n",
    "# 주가 데이터 사전 준비\n",
    "stock_data = {'005930': stock_df}\n",
    "\n",
    "print(\"데이터 전처리 완료\")\n",
    "print(f\"뉴스 데이터 날짜 범위: {news_df['date'].min()} ~ {news_df['date'].max()}\")\n",
    "print(f\"주가 데이터 날짜 범위: {stock_df['date'].min()} ~ {stock_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 전처리\n",
    "\n",
    "뉴스 기사의 텍스트를 전처리합니다. 이 단계에서는 다음 작업을 수행합니다:\n",
    "- 토큰화 (Mecab을 사용한 한국어 형태소 분석)\n",
    "- 불용어 제거\n",
    "- 특수 문자 및 숫자 처리\n",
    "- 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리기 초기화\n",
    "text_preprocessor = TextPreprocessor(\n",
    "    language='ko',\n",
    "    use_mecab=True,\n",
    "    remove_stopwords=True,\n",
    "    min_token_length=2\n",
    ")\n",
    "\n",
    "# 전처리 예시 - 단일 텍스트\n",
    "sample_text = news_df.iloc[0]['Body']\n",
    "print(\"원본 텍스트 샘플:\")\n",
    "print(sample_text[:300], \"...\")\n",
    "\n",
    "processed_text = text_preprocessor.preprocess_text(sample_text)\n",
    "print(\"\\n전처리된 텍스트:\")\n",
    "print(processed_text[:300], \"...\")\n",
    "\n",
    "# 키워드 추출 예시\n",
    "keywords = text_preprocessor.extract_keywords(sample_text, top_n=10)\n",
    "print(\"\\n추출된 키워드:\")\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 뉴스 데이터 전처리\n",
    "\n",
    "이제 전체 뉴스 데이터셋에 전처리를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 뉴스 데이터 전처리\n",
    "print(\"전체 뉴스 데이터 전처리 중...\")\n",
    "\n",
    "# 제목 및 본문 전처리\n",
    "preprocessed_df = text_preprocessor.preprocess_dataframe(\n",
    "    news_df,\n",
    "    title_col='Title',\n",
    "    body_col='Body',\n",
    "    summary_col='AI Summary',\n",
    "    use_summary=True\n",
    ")\n",
    "\n",
    "# 키워드 추출\n",
    "preprocessed_df = text_preprocessor.extract_keywords_from_df(\n",
    "    preprocessed_df,\n",
    "    text_col='processed_content',\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "print(\"전처리 완료\")\n",
    "print(f\"전처리된 기사: {len(preprocessed_df)}개\")\n",
    "print(\"\\n전처리된 데이터 샘플:\")\n",
    "display(preprocessed_df[['Title', 'processed_title', 'keywords']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 뉴스 벡터화\n",
    "\n",
    "전처리된 뉴스 기사를 벡터 표현으로 변환합니다. 이 단계에서는 다음 작업을 수행합니다:\n",
    "- TF-IDF 벡터화 (기본 방법)\n",
    "- 제목과 내용의 가중치 조정\n",
    "- 차원 축소 (선택 사항)\n",
    "\n",
    "### 벡터화 방법 설명\n",
    "\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency)는 문서 집합에서 단어의 중요도를 계산하는 방법입니다.\n",
    "- TF(Term Frequency): 특정 단어가 문서에 등장하는 빈도\n",
    "- IDF(Inverse Document Frequency): 전체 문서 집합에서 특정 단어를 포함하는 문서의 수에 반비례하는 값\n",
    "\n",
    "수식:\n",
    "- $TF(t, d) = \\frac{\\text{단어 t가 문서 d에 등장한 횟수}}{\\text{문서 d의 총 단어 수}}$\n",
    "- $IDF(t) = \\log\\frac{\\text{전체 문서 수}}{\\text{단어 t가 등장한 문서 수}}$\n",
    "- $TFIDF(t, d) = TF(t, d) \\times IDF(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 벡터화기 초기화\n",
    "news_vectorizer = NewsVectorizer(\n",
    "    method='tfidf',\n",
    "    max_features=10000,\n",
    "    embedding_dim=300,\n",
    "    use_gpu=True,\n",
    "    title_weight=2.0\n",
    ")\n",
    "\n",
    "# 벡터화 수행\n",
    "print(\"뉴스 기사 벡터화 중...\")\n",
    "vectorized_df = news_vectorizer.vectorize_articles(\n",
    "    preprocessed_df,\n",
    "    content_col='processed_content',\n",
    "    title_col='processed_title',\n",
    "    combine_title_content=True,\n",
    "    reduce_dims=True,\n",
    "    n_components=100\n",
    ")\n",
    "\n",
    "print(\"벡터화 완료\")\n",
    "print(f\"벡터화된 기사: {len(vectorized_df)}개\")\n",
    "\n",
    "# 벡터 차원 확인\n",
    "vector_sample = vectorized_df['vector'].iloc[0]\n",
    "print(f\"벡터 차원: {len(vector_sample)}\")\n",
    "\n",
    "# 벡터 시각화 (처음 10개 차원)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(10), vector_sample[:10])\n",
    "plt.title('첫 번째 기사 벡터의 처음 10개 차원')\n",
    "plt.xlabel('차원 인덱스')\n",
    "plt.ylabel('값')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 유사도 시각화\n",
    "\n",
    "기사 간의 유사도를 시각화하여 벡터화의 효과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 샘플 기사 선택 (처음 10개)\n",
    "sample_vectors = np.array(vectorized_df['vector'].iloc[:10].tolist())\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity_matrix = cosine_similarity(sample_vectors)\n",
    "\n",
    "# 유사도 히트맵 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, annot=True, cmap='YlGnBu', vmin=0, vmax=1)\n",
    "plt.title('기사 간 코사인 유사도')\n",
    "plt.xlabel('기사 인덱스')\n",
    "plt.ylabel('기사 인덱스')\n",
    "plt.show()\n",
    "\n",
    "# 가장 유사한 기사 쌍 찾기\n",
    "np.fill_diagonal(similarity_matrix, 0)  # 자기 자신과의 유사도 제외\n",
    "max_sim_idx = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n",
    "print(f\"가장 유사한 기사 쌍: {max_sim_idx[0]}번과 {max_sim_idx[1]}번 (유사도: {similarity_matrix[max_sim_idx]:.4f})\")\n",
    "\n",
    "# 유사한 기사 제목 출력\n",
    "print(f\"\\n기사 {max_sim_idx[0]}번 제목: {vectorized_df['Title'].iloc[max_sim_idx[0]]}\")\n",
    "print(f\"기사 {max_sim_idx[1]}번 제목: {vectorized_df['Title'].iloc[max_sim_idx[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 뉴스 클러스터링\n",
    "\n",
    "벡터화된 뉴스 기사를 클러스터링하여 유사한 기사를 그룹화합니다. 이 단계에서는 다음 작업을 수행합니다:\n",
    "- 계층적 응집 클러스터링 (HAC) 적용\n",
    "- 클러스터 주제 생성\n",
    "- 트렌드 클러스터 식별\n",
    "\n",
    "### 클러스터링 알고리즘 설명\n",
    "\n",
    "계층적 응집 클러스터링(HAC)은 네이버 AiRs 시스템에서도 사용하는 방법으로, 다음과 같은 단계로 진행됩니다:\n",
    "1. 각 기사를 개별 클러스터로 시작\n",
    "2. 가장 유사한 두 클러스터를 병합\n",
    "3. 거리 임계값에 도달할 때까지 2단계 반복\n",
    "\n",
    "거리 계산 공식 (코사인 거리):\n",
    "- $distance(A, B) = 1 - \\frac{A \\cdot B}{||A|| \\cdot ||B||}$\n",
    "\n",
    "여기서 $A$와 $B$는 기사 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 클러스터링 초기화\n",
    "news_clustering = NewsClustering(\n",
    "    distance_threshold=0.7,  # 거리 임계값 (낮을수록 더 엄격한 클러스터링)\n",
    "    min_cluster_size=2,      # 최소 클러스터 크기\n",
    "    max_cluster_size=20,     # 최대 클러스터 크기\n",
    "    linkage='average'        # 연결 방법 (평균 연결)\n",
    ")\n",
    "\n",
    "# 클러스터링 수행\n",
    "print(\"뉴스 기사 클러스터링 중...\")\n",
    "clustered_df = news_clustering.cluster_articles(\n",
    "    vectorized_df,\n",
    "    vector_col='vector',\n",
    "    title_col='Title',\n",
    "    content_col='Body'\n",
    ")\n",
    "\n",
    "print(\"클러스터링 완료\")\n",
    "print(f\"생성된 클러스터 수: {clustered_df['cluster_id'].nunique()}\")\n",
    "\n",
    "# 클러스터 크기 분포\n",
    "cluster_sizes = clustered_df['cluster_id'].value_counts().sort_index()\n",
    "print(\"\\n클러스터 크기 분포:\")\n",
    "display(cluster_sizes.head(10))\n",
    "\n",
    "# 클러스터 크기 히스토그램\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(cluster_sizes.values, bins=20)\n",
    "plt.title('클러스터 크기 분포')\n",
    "plt.xlabel('클러스터 크기')\n",
    "plt.ylabel('클러스터 수')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터 주제 및 내용 확인\n",
    "\n",
    "생성된 클러스터의 주제와 포함된 기사를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 주제 생성\n",
    "clustered_df = news_clustering.generate_cluster_topics(clustered_df)\n",
    "\n",
    "# 클러스터 주제 확인\n",
    "cluster_topics = clustered_df.groupby('cluster_id')['cluster_topic'].first()\n",
    "print(\"클러스터 주제 샘플:\")\n",
    "display(cluster_topics.head(5))\n",
    "\n",
    "# 특정 클러스터의 기사 확인\n",
    "sample_cluster_id = cluster_sizes.index[0]  # 첫 번째 클러스터 선택\n",
    "sample_cluster = clustered_df[clustered_df['cluster_id'] == sample_cluster_id]\n",
    "\n",
    "print(f\"\\n클러스터 {sample_cluster_id}번 (주제: {sample_cluster['cluster_topic'].iloc[0]})의 기사:\")\n",
    "display(sample_cluster[['Title', 'Date', 'Press']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트렌드 클러스터 식별\n",
    "\n",
    "최근 24시간 내에 많은 기사가 포함된 클러스터를 트렌드 클러스터로 식별합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트렌드 클러스터 식별\n",
    "trending_clusters = news_clustering.get_trending_clusters(\n",
    "    clustered_df,\n",
    "    timeframe_hours=24,\n",
    "    min_articles=2\n",
    ")\n",
    "\n",
    "print(f\"식별된 트렌드 클러스터: {len(trending_clusters)}개\")\n",
    "print(\"\\n트렌드 클러스터 주제:\")\n",
    "for cluster_id, info in trending_clusters.items():\n",
    "    print(f\"클러스터 {cluster_id}번: {info['topic']} (기사 {info['count']}개)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 주가 영향 분석\n",
    "\n",
    "뉴스 기사가 주가에 미치는 영향을 분석합니다. 이 단계는 네이버 AiRs의 개인화 구성 요소를 대체하는 CLEAR 시스템의 핵심 부분입니다.\n",
    "\n",
    "### 영향 분석 방법 설명\n",
    "\n",
    "주가 영향 분석은 다음 단계로 진행됩니다:\n",
    "1. 각 뉴스 기사에 대해 발행 시간 식별\n",
    "2. 여러 시간 윈도우(즉시, 단기, 중기)에서 주가 변화 계산\n",
    "3. 가격 및 거래량 변화를 기반으로 영향 점수 계산\n",
    "\n",
    "영향 점수 계산 공식:\n",
    "- $Impact_{price} = \\frac{Price_{after} - Price_{before}}{Price_{before}} \\times 100$\n",
    "- $Impact_{volume} = \\frac{Volume_{after} - Volume_{avg}}{Volume_{avg}} \\times 100$\n",
    "- $Impact_{total} = w_{price} \\times Impact_{price} + w_{volume} \\times Impact_{volume}$\n",
    "\n",
    "최종 영향 점수는 -5에서 +5 사이의 척도로 정규화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주가 영향 분석기 초기화\n",
    "impact_analyzer = StockImpactAnalyzer(\n",
    "    time_windows=[\n",
    "        {\"name\": \"immediate\", \"hours\": 1},\n",
    "        {\"name\": \"short_term\", \"hours\": 24},\n",
    "        {\"name\": \"medium_term\", \"days\": 3}\n",
    "    ],\n",
    "    impact_thresholds={\n",
    "        \"high\": 0.02,    # 2% 가격 변동\n",
    "        \"medium\": 0.01,  # 1% 가격 변동\n",
    "        \"low\": 0.005     # 0.5% 가격 변동\n",
    "    },\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "# 영향 분석 수행\n",
    "print(\"주가 영향 분석 중...\")\n",
    "impact_df = impact_analyzer.analyze_news_impact(\n",
    "    clustered_df,\n",
    "    stock_data\n",
    ")\n",
    "\n",
    "print(\"영향 분석 완료\")\n",
    "print(f\"영향 점수가 계산된 기사: {len(impact_df.dropna(subset=['impact_score']))}개\")\n",
    "\n",
    "# 영향 점수 분포\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(impact_df['impact_score'].dropna(), bins=20, kde=True)\n",
    "plt.title('주가 영향 점수 분포')\n",
    "plt.xlabel('영향 점수 (-5 ~ +5)')\n",
    "plt.ylabel('기사 수')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# 영향 점수 통계\n",
    "impact_stats = impact_df['impact_score'].describe()\n",
    "print(\"\\n영향 점수 통계:\")\n",
    "display(impact_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영향 예측 모델 훈련\n",
    "\n",
    "과거 데이터를 기반으로 뉴스 기사의 주가 영향을 예측하는 모델을 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영향 예측 모델 훈련\n",
    "print(\"영향 예측 모델 훈련 중...\")\n",
    "impact_analyzer.train_impact_model(\n",
    "    impact_df,\n",
    "    stock_data,\n",
    "    model_type='random_forest'\n",
    ")\n",
    "\n",
    "# 예측 수행\n",
    "impact_df = impact_analyzer.predict_impact(\n",
    "    impact_df,\n",
    "    include_features=True\n",
    ")\n",
    "\n",
    "print(\"예측 완료\")\n",
    "print(f\"예측된 영향 점수가 있는 기사: {len(impact_df.dropna(subset=['predicted_impact']))}개\")\n",
    "\n",
    "# 실제 vs 예측 영향 점수 비교\n",
    "valid_impact_df = impact_df.dropna(subset=['impact_score', 'predicted_impact'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(valid_impact_df['impact_score'], valid_impact_df['predicted_impact'], alpha=0.6)\n",
    "plt.plot([-5, 5], [-5, 5], 'r--')\n",
    "plt.title('실제 vs 예측 영향 점수')\n",
    "plt.xlabel('실제 영향 점수')\n",
    "plt.ylabel('예측 영향 점수')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 예측 성능 지표\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_true = valid_impact_df['impact_score']\n",
    "y_pred = valid_impact_df['predicted_impact']\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\n예측 성능 지표:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주가 영향 시각화\n",
    "\n",
    "뉴스 기사와 주가 변동 간의 관계를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주가 영향 시각화\n",
    "ticker = '005930'  # 삼성전자\n",
    "viz_path = impact_analyzer.visualize_impact(\n",
    "    impact_df,\n",
    "    stock_data,\n",
    "    ticker,\n",
    "    save_path=None  # 저장하지 않고 직접 표시\n",
    ")\n",
    "\n",
    "# 영향이 큰 상위 기사 확인\n",
    "print(\"\\n영향이 가장 큰 긍정적 기사 (상위 3개):\")\n",
    "display(impact_df.sort_values('impact_score', ascending=False)[['Title', 'Date', 'impact_score']].head(3))\n",
    "\n",
    "print(\"\\n영향이 가장 큰 부정적 기사 (상위 3개):\")\n",
    "display(impact_df.sort_values('impact_score')[['Title', 'Date', 'impact_score']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 뉴스 추천\n",
    "\n",
    "주가 영향 및 기타 요소를 기반으로 뉴스 기사를 추천합니다. 이 단계에서는 네이버 AiRs의 다중 요소 접근 방식을 유지하되, 개인화 대신 주가 영향에 중점을 둡니다.\n",
    "\n",
    "### 추천 알고리즘 설명\n",
    "\n",
    "CLEAR 추천 시스템은 다음 5가지 요소를 기반으로 점수를 계산합니다:\n",
    "1. **주가 영향(SI)**: AiRs의 사회적 관심도를 대체하여 중요한 재무적 영향이 있는 기사 우선순위\n",
    "2. **품질 평가(QE)**: 클러스터 크기 및 기타 지표를 기사 품질의 대리 지표로 사용\n",
    "3. **콘텐츠 기반 필터링(CBF)**: 인기 있는 금융 기사와의 유사성 측정\n",
    "4. **협업 필터링(CF)**: 기사 관계에 NPMI(정규화된 점별 상호 정보량) 사용\n",
    "5. **최신**: 시기적절한 추천을 보장하기 위해 최근 기사 우선순위\n",
    "\n",
    "최종 추천 점수 계산 공식:\n",
    "- $Score = w_{impact} \\times SI + w_{quality} \\times QE + w_{content} \\times CBF + w_{collaborative} \\times CF + w_{recency} \\times Latest$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 추천기 초기화\n",
    "news_recommender = NewsRecommender(\n",
    "    weights={\n",
    "        'impact': 0.4,      # 주가 영향(SI)\n",
    "        'quality': 0.2,     # 품질 평가(QE)\n",
    "        'content': 0.2,     # 콘텐츠 기반 필터링(CBF)\n",
    "        'collaborative': 0.1, # 협업 필터링(CF)\n",
    "        'recency': 0.1      # 최신 뉴스 우선순위\n",
    "    }\n",
    ")\n",
    "\n",
    "# 기사 추천 생성\n",
    "print(\"뉴스 추천 생성 중...\")\n",
    "top_articles = news_recommender.recommend_articles(\n",
    "    impact_df,\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "print(\"추천 생성 완료\")\n",
    "print(f\"추천된 기사: {len(top_articles)}개\")\n",
    "\n",
    "# 추천 기사 확인\n",
    "print(\"\\n추천 기사 목록:\")\n",
    "display(top_articles[['Title', 'Date', 'recommendation_score', 'impact_score', 'cluster_topic']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터 기반 추천\n",
    "\n",
    "클러스터 단위로 추천을 생성하여 다양한 주제의 뉴스를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 추천 생성\n",
    "top_clusters = news_recommender.recommend_clusters(\n",
    "    impact_df,\n",
    "    top_n=5,\n",
    "    articles_per_cluster=3\n",
    ")\n",
    "\n",
    "print(f\"추천된 클러스터: {len(top_clusters)}개\")\n",
    "\n",
    "# 각 클러스터의 추천 기사 확인\n",
    "for cluster_id, cluster_df in top_clusters.items():\n",
    "    cluster_topic = cluster_df['cluster_topic'].iloc[0]\n",
    "    print(f\"\\n클러스터 {cluster_id}번 (주제: {cluster_topic})의 추천 기사:\")\n",
    "    display(cluster_df[['Title', 'Date', 'recommendation_score', 'impact_score']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트렌드 주제 추천\n",
    "\n",
    "현재 트렌드인 주제를 추천합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트렌드 주제 추천\n",
    "trending_topics = news_recommender.recommend_trending_topics(\n",
    "    impact_df,\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "print(f\"추천된 트렌드 주제: {len(trending_topics)}개\")\n",
    "print(\"\\n트렌드 주제 목록:\")\n",
    "for i, topic in enumerate(trending_topics, 1):\n",
    "    print(f\"{i}. {topic['topic']} (점수: {topic['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 시각화 및 평가\n",
    "\n",
    "시스템의 성능을 평가하고 결과를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가기 초기화\n",
    "evaluator = CLEAREvaluator(results_dir=\"results/evaluation\")\n",
    "\n",
    "# 클러스터링 평가\n",
    "print(\"클러스터링 평가 중...\")\n",
    "clustering_metrics = evaluator.evaluate_clustering(\n",
    "    impact_df,\n",
    "    vector_col='vector',\n",
    "    cluster_col='cluster_id'\n",
    ")\n",
    "\n",
    "print(\"\\n클러스터링 평가 지표:\")\n",
    "for metric, value in clustering_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\" if isinstance(value, float) else f\"{metric}: {value}\")\n",
    "\n",
    "# 영향 예측 평가\n",
    "print(\"\\n영향 예측 평가 중...\")\n",
    "impact_metrics = evaluator.evaluate_impact_prediction(\n",
    "    impact_df,\n",
    "    actual_col='impact_score',\n",
    "    predicted_col='predicted_impact'\n",
    ")\n",
    "\n",
    "print(\"\\n영향 예측 평가 지표:\")\n",
    "for metric, value in impact_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# 추천 평가\n",
    "print(\"\\n추천 평가 중...\")\n",
    "recommendations = {\n",
    "    'top_articles': top_articles,\n",
    "    'top_clusters': top_clusters,\n",
    "    'trending_topics': trending_topics\n",
    "}\n",
    "recommendation_metrics = evaluator.evaluate_recommendations(recommendations)\n",
    "\n",
    "print(\"\\n추천 평가 지표:\")\n",
    "for metric, value in recommendation_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\" if isinstance(value, float) else f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종합 결과 시각화\n",
    "\n",
    "시스템의 주요 결과를 종합적으로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 결과 시각화\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. 클러스터 크기 분포\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(cluster_sizes.values, bins=20)\n",
    "plt.title('클러스터 크기 분포')\n",
    "plt.xlabel('클러스터 크기')\n",
    "plt.ylabel('클러스터 수')\n",
    "\n",
    "# 2. 영향 점수 분포\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(impact_df['impact_score'].dropna(), bins=20, kde=True)\n",
    "plt.title('주가 영향 점수 분포')\n",
    "plt.xlabel('영향 점수 (-5 ~ +5)')\n",
    "plt.ylabel('기사 수')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "# 3. 실제 vs 예측 영향 점수\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(valid_impact_df['impact_score'], valid_impact_df['predicted_impact'], alpha=0.6)\n",
    "plt.plot([-5, 5], [-5, 5], 'r--')\n",
    "plt.title('실제 vs 예측 영향 점수')\n",
    "plt.xlabel('실제 영향 점수')\n",
    "plt.ylabel('예측 영향 점수')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(True)\n",
    "\n",
    "# 4. 추천 점수 분포\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(top_articles['recommendation_score'], bins=20, kde=True)\n",
    "plt.title('추천 점수 분포')\n",
    "plt.xlabel('추천 점수')\n",
    "plt.ylabel('기사 수')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "이 노트북에서는 CLEAR 시스템의 각 모듈을 단계별로 시연했습니다. CLEAR는 네이버의 AiRs 아키텍처를 기반으로 하되, 개인화 대신 주가 영향에 중점을 둔 뉴스 추천 시스템입니다.\n",
    "\n",
    "주요 구성 요소:\n",
    "1. **텍스트 전처리**: Mecab을 사용한 한국어 형태소 분석 및 불용어 제거\n",
    "2. **뉴스 벡터화**: TF-IDF 및 기타 임베딩 방법을 사용한 벡터 표현 생성\n",
    "3. **뉴스 클러스터링**: 계층적 응집 클러스터링을 사용한 유사 기사 그룹화\n",
    "4. **주가 영향 분석**: 뉴스 기사가 주가에 미치는 영향 측정 및 예측\n",
    "5. **뉴스 추천**: 주가 영향 및 기타 요소를 기반으로 한 다중 요소 추천\n",
    "\n",
    "CLEAR 시스템은 AiRs의 핵심 메커니즘을 유지하면서 금융 도메인에 맞게 조정되어, 투자자와 금융 분석가에게 가치 있는 도구를 제공합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
