{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAR System Demo Notebook\n",
    "\n",
    "## NAVER AiRS 기반 뉴스 추천 및 주식 영향 분석 시스템\n",
    "\n",
    "이 노트북은 NAVER의 AiRS 알고리즘을 기반으로 한 뉴스 추천 및 주식 영향 분석 시스템인 CLEAR의 기능을 시연합니다. 이 시스템은 뉴스 클러스터링, 추천, 그리고 주식 가격에 미치는 영향을 분석하는 기능을 포함합니다.\n",
    "\n",
    "This notebook demonstrates the functionality of CLEAR, a news recommendation and stock impact analysis system based on NAVER's AiRS algorithm. The system includes news clustering, recommendation, and analysis of impact on stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 시스템 개요 (System Overview)\n",
    "\n",
    "CLEAR 시스템은 다음과 같은 주요 구성 요소로 이루어져 있습니다:\n",
    "1. 데이터 처리 (뉴스 및 주식 데이터)\n",
    "2. 텍스트 전처리\n",
    "3. 뉴스 벡터화\n",
    "4. 뉴스 클러스터링\n",
    "5. 주식 영향 분석\n",
    "6. 뉴스 추천\n",
    "\n",
    "The CLEAR system consists of the following main components:\n",
    "1. Data processing (news and stock data)\n",
    "2. Text preprocessing\n",
    "3. News vectorization\n",
    "4. News clustering\n",
    "5. Stock impact analysis\n",
    "6. News recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAR 시스템 데모 환경이 준비되었습니다.\n",
      "CLEAR system demo environment is ready.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 숨기기\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시스템 경로에 src 디렉토리 추가\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"CLEAR 시스템 데모 환경이 준비되었습니다.\")\n",
    "print(\"CLEAR system demo environment is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 (Data Loading)\n",
    "\n",
    "먼저 뉴스 데이터와 주식 데이터를 로드합니다. 이 데이터는 시스템의 기본 입력으로 사용됩니다.\n",
    "\n",
    "First, we load the news data and stock data. This data is used as the basic input for the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 데이터 샘플 수: 32208\n",
      "주식 데이터 샘플 수: 2856\n",
      "\n",
      "뉴스 데이터 샘플:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Press</th>\n",
       "      <th>Link</th>\n",
       "      <th>Body</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Num_comment</th>\n",
       "      <th>AI Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>세계 2위 휴대전화 시장 인도, 이제 중·고가 제품에 눈 돌린다</td>\n",
       "      <td>20190101 15:36</td>\n",
       "      <td>yna</td>\n",
       "      <td>https://www.yna.co.kr/view/AKR20190101051400077</td>\n",
       "      <td>김영현기자\\n구독\\n\"저가에서 고급 브랜드로 관심 옮겨…5년간 4배 성장 예상\"\\n...</td>\n",
       "      <td>{'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[부고] 홍현칠(삼성전자 서남아총괄)씨 부친상</td>\n",
       "      <td>20190101 14:53</td>\n",
       "      <td>yna</td>\n",
       "      <td>https://www.yna.co.kr/view/AKR20190101048900077</td>\n",
       "      <td>김영현기자\\n구독\\n▲ 홍선기씨 별세, 홍현구·현칠(삼성전자 서남아총괄 부사장)·현...</td>\n",
       "      <td>{'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SK그룹, CES서 신사업 기회 모색…경영진 총출동</td>\n",
       "      <td>20190101 09:00</td>\n",
       "      <td>yna</td>\n",
       "      <td>https://www.yna.co.kr/view/AKR20181231260700003</td>\n",
       "      <td>배영경기자\\n구독\\n(서울=연합뉴스) 배영경 기자 = SK이노베이션·SK텔레콤·SK...</td>\n",
       "      <td>{'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'알박기 노조'에 미행까지…삼성에버랜드 노조방해 13명 기소</td>\n",
       "      <td>20190101 09:00</td>\n",
       "      <td>yna</td>\n",
       "      <td>https://www.yna.co.kr/view/AKR20181231271300004</td>\n",
       "      <td>김계연기자\\n구독\\n교섭창구 단일화 악용해 어용노조 결성…설립신고서 대리작성\\n조합...</td>\n",
       "      <td>{'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CES 달굴 모바일은…LG·소니 중저가폰에 로욜 폴더블폰도</td>\n",
       "      <td>20190101 08:10</td>\n",
       "      <td>yna</td>\n",
       "      <td>https://www.yna.co.kr/view/AKR20181231240500017</td>\n",
       "      <td>채새롬기자\\n구독\\n삼성전자는 갤노트9·갤럭시홈 전시…갤럭시홈 출시 일정은 미정\\n...</td>\n",
       "      <td>{'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title            Date Press  \\\n",
       "0  세계 2위 휴대전화 시장 인도, 이제 중·고가 제품에 눈 돌린다  20190101 15:36   yna   \n",
       "1            [부고] 홍현칠(삼성전자 서남아총괄)씨 부친상  20190101 14:53   yna   \n",
       "2         SK그룹, CES서 신사업 기회 모색…경영진 총출동  20190101 09:00   yna   \n",
       "3    '알박기 노조'에 미행까지…삼성에버랜드 노조방해 13명 기소  20190101 09:00   yna   \n",
       "4     CES 달굴 모바일은…LG·소니 중저가폰에 로욜 폴더블폰도  20190101 08:10   yna   \n",
       "\n",
       "                                              Link  \\\n",
       "0  https://www.yna.co.kr/view/AKR20190101051400077   \n",
       "1  https://www.yna.co.kr/view/AKR20190101048900077   \n",
       "2  https://www.yna.co.kr/view/AKR20181231260700003   \n",
       "3  https://www.yna.co.kr/view/AKR20181231271300004   \n",
       "4  https://www.yna.co.kr/view/AKR20181231240500017   \n",
       "\n",
       "                                                Body  \\\n",
       "0  김영현기자\\n구독\\n\"저가에서 고급 브랜드로 관심 옮겨…5년간 4배 성장 예상\"\\n...   \n",
       "1  김영현기자\\n구독\\n▲ 홍선기씨 별세, 홍현구·현칠(삼성전자 서남아총괄 부사장)·현...   \n",
       "2  배영경기자\\n구독\\n(서울=연합뉴스) 배영경 기자 = SK이노베이션·SK텔레콤·SK...   \n",
       "3  김계연기자\\n구독\\n교섭창구 단일화 악용해 어용노조 결성…설립신고서 대리작성\\n조합...   \n",
       "4  채새롬기자\\n구독\\n삼성전자는 갤노트9·갤럭시홈 전시…갤럭시홈 출시 일정은 미정\\n...   \n",
       "\n",
       "                                             Emotion  Num_comment AI Summary  \n",
       "0  {'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...          NaN        NaN  \n",
       "1  {'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...          NaN        NaN  \n",
       "2  {'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...          NaN        NaN  \n",
       "3  {'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...          NaN        NaN  \n",
       "4  {'GOOD': '0', 'SAD': '0', 'ANGRY': '0', 'NEXT'...          NaN        NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 데이터 로드\n",
    "news_data_path = os.path.join(os.getcwd(), 'data', 'news', 'yna_005930_all.csv')\n",
    "news_df = pd.read_csv(news_data_path)\n",
    "\n",
    "# 주식 데이터 로드\n",
    "stock_data_path = os.path.join(os.getcwd(), 'data', 'stock', 'stockprice_005930.csv')\n",
    "stock_df = pd.read_csv(stock_data_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"뉴스 데이터 샘플 수: {len(news_df)}\")\n",
    "print(f\"주식 데이터 샘플 수: {len(stock_df)}\")\n",
    "\n",
    "# 뉴스 데이터 샘플 확인\n",
    "print(\"\\n뉴스 데이터 샘플:\")\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주식 데이터 샘플:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Start</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>End</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250305</td>\n",
       "      <td>0</td>\n",
       "      <td>55100</td>\n",
       "      <td>55400</td>\n",
       "      <td>54200</td>\n",
       "      <td>54300</td>\n",
       "      <td>6367338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250304</td>\n",
       "      <td>0</td>\n",
       "      <td>53900</td>\n",
       "      <td>55000</td>\n",
       "      <td>53800</td>\n",
       "      <td>54500</td>\n",
       "      <td>18553933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250228</td>\n",
       "      <td>0</td>\n",
       "      <td>55400</td>\n",
       "      <td>55700</td>\n",
       "      <td>54500</td>\n",
       "      <td>54500</td>\n",
       "      <td>28036698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250227</td>\n",
       "      <td>0</td>\n",
       "      <td>56500</td>\n",
       "      <td>57100</td>\n",
       "      <td>56200</td>\n",
       "      <td>56300</td>\n",
       "      <td>14975356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250226</td>\n",
       "      <td>0</td>\n",
       "      <td>57000</td>\n",
       "      <td>57100</td>\n",
       "      <td>56100</td>\n",
       "      <td>56600</td>\n",
       "      <td>18117091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20250225</td>\n",
       "      <td>0</td>\n",
       "      <td>56600</td>\n",
       "      <td>57800</td>\n",
       "      <td>56500</td>\n",
       "      <td>57200</td>\n",
       "      <td>14625181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20250224</td>\n",
       "      <td>0</td>\n",
       "      <td>57300</td>\n",
       "      <td>57700</td>\n",
       "      <td>57200</td>\n",
       "      <td>57300</td>\n",
       "      <td>14138471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20250221</td>\n",
       "      <td>0</td>\n",
       "      <td>58400</td>\n",
       "      <td>58500</td>\n",
       "      <td>57100</td>\n",
       "      <td>58200</td>\n",
       "      <td>22198428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20250220</td>\n",
       "      <td>0</td>\n",
       "      <td>59100</td>\n",
       "      <td>59100</td>\n",
       "      <td>58100</td>\n",
       "      <td>58400</td>\n",
       "      <td>21589059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20250219</td>\n",
       "      <td>0</td>\n",
       "      <td>57400</td>\n",
       "      <td>58900</td>\n",
       "      <td>57300</td>\n",
       "      <td>58700</td>\n",
       "      <td>25395151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20250218</td>\n",
       "      <td>0</td>\n",
       "      <td>56200</td>\n",
       "      <td>57200</td>\n",
       "      <td>55900</td>\n",
       "      <td>56900</td>\n",
       "      <td>22131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20250217</td>\n",
       "      <td>0</td>\n",
       "      <td>56100</td>\n",
       "      <td>56500</td>\n",
       "      <td>55700</td>\n",
       "      <td>56000</td>\n",
       "      <td>11916027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20250214</td>\n",
       "      <td>0</td>\n",
       "      <td>56000</td>\n",
       "      <td>57300</td>\n",
       "      <td>56000</td>\n",
       "      <td>56000</td>\n",
       "      <td>23979779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20250213</td>\n",
       "      <td>0</td>\n",
       "      <td>56100</td>\n",
       "      <td>56400</td>\n",
       "      <td>55600</td>\n",
       "      <td>55800</td>\n",
       "      <td>22448377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20250212</td>\n",
       "      <td>0</td>\n",
       "      <td>55100</td>\n",
       "      <td>55900</td>\n",
       "      <td>54500</td>\n",
       "      <td>55800</td>\n",
       "      <td>26428596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20250211</td>\n",
       "      <td>0</td>\n",
       "      <td>55500</td>\n",
       "      <td>56300</td>\n",
       "      <td>55000</td>\n",
       "      <td>55700</td>\n",
       "      <td>24596196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20250210</td>\n",
       "      <td>0</td>\n",
       "      <td>53000</td>\n",
       "      <td>55900</td>\n",
       "      <td>52900</td>\n",
       "      <td>55600</td>\n",
       "      <td>27577591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20250207</td>\n",
       "      <td>0</td>\n",
       "      <td>53900</td>\n",
       "      <td>54200</td>\n",
       "      <td>53600</td>\n",
       "      <td>53700</td>\n",
       "      <td>14476866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20250206</td>\n",
       "      <td>0</td>\n",
       "      <td>53400</td>\n",
       "      <td>54000</td>\n",
       "      <td>53200</td>\n",
       "      <td>54000</td>\n",
       "      <td>16466025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20250205</td>\n",
       "      <td>0</td>\n",
       "      <td>53600</td>\n",
       "      <td>53800</td>\n",
       "      <td>52800</td>\n",
       "      <td>52900</td>\n",
       "      <td>15974885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Time  Start   High    Low    End    Volume\n",
       "0   20250305     0  55100  55400  54200  54300   6367338\n",
       "1   20250304     0  53900  55000  53800  54500  18553933\n",
       "2   20250228     0  55400  55700  54500  54500  28036698\n",
       "3   20250227     0  56500  57100  56200  56300  14975356\n",
       "4   20250226     0  57000  57100  56100  56600  18117091\n",
       "5   20250225     0  56600  57800  56500  57200  14625181\n",
       "6   20250224     0  57300  57700  57200  57300  14138471\n",
       "7   20250221     0  58400  58500  57100  58200  22198428\n",
       "8   20250220     0  59100  59100  58100  58400  21589059\n",
       "9   20250219     0  57400  58900  57300  58700  25395151\n",
       "10  20250218     0  56200  57200  55900  56900  22131007\n",
       "11  20250217     0  56100  56500  55700  56000  11916027\n",
       "12  20250214     0  56000  57300  56000  56000  23979779\n",
       "13  20250213     0  56100  56400  55600  55800  22448377\n",
       "14  20250212     0  55100  55900  54500  55800  26428596\n",
       "15  20250211     0  55500  56300  55000  55700  24596196\n",
       "16  20250210     0  53000  55900  52900  55600  27577591\n",
       "17  20250207     0  53900  54200  53600  53700  14476866\n",
       "18  20250206     0  53400  54000  53200  54000  16466025\n",
       "19  20250205     0  53600  53800  52800  52900  15974885"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주식 데이터 샘플 확인\n",
    "print(\"주식 데이터 샘플:\")\n",
    "stock_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "뉴스 데이터와 주식 데이터를 분석에 적합한 형태로 전처리합니다.\n",
    "\n",
    "We preprocess the news data and stock data into a form suitable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U1')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m news_df \u001b[38;5;241m=\u001b[39m news_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 주식 데이터 전처리\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m stock_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mstock_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m \u001b[38;5;241m+\u001b[39m stock_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m stock_df \u001b[38;5;241m=\u001b[39m stock_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 주식 가격 변동 계산\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/arraylike.py:186\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__add__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Get Addition of DataFrame and other, column-wise.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    moose     3.0     NaN\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/series.py:6126\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6125\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    215\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U1')) -> None"
     ]
    }
   ],
   "source": [
    "# 날짜 형식 변환 함수\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        # 예: '20250101 18:56' -> '2025-01-01 18:56:00'\n",
    "        date_parts = date_str.split(' ')\n",
    "        date = date_parts[0]\n",
    "        time = date_parts[1] if len(date_parts) > 1 else '00:00'\n",
    "        \n",
    "        year = date[:4]\n",
    "        month = date[4:6]\n",
    "        day = date[6:8]\n",
    "        \n",
    "        return f\"{year}-{month}-{day} {time}:00\"\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 변환 오류: {e} - {date_str}\")\n",
    "        return None\n",
    "\n",
    "# 뉴스 데이터 전처리\n",
    "news_df['Date'] = news_df['Date'].apply(convert_date_format)\n",
    "news_df['datetime'] = pd.to_datetime(news_df['Date'])\n",
    "news_df = news_df.sort_values('datetime')\n",
    "\n",
    "# 주식 데이터 전처리\n",
    "stock_df['datetime'] = pd.to_datetime(stock_df['Date'] + ' ' + stock_df['Time'])\n",
    "stock_df = stock_df.sort_values('datetime')\n",
    "\n",
    "# 주식 가격 변동 계산\n",
    "stock_df['price_change'] = stock_df['End'] - stock_df['Start']\n",
    "stock_df['price_change_pct'] = (stock_df['price_change'] / stock_df['Start']) * 100\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(\"전처리된 뉴스 데이터:\")\n",
    "news_df[['Title', 'datetime', 'Press']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 주식 데이터:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['datetime', 'price_change', 'price_change_pct'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 전처리된 주식 데이터 확인\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m전처리된 주식 데이터:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstock_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice_change\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice_change_pct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dl/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['datetime', 'price_change', 'price_change_pct'] not in index\""
     ]
    }
   ],
   "source": [
    "# 전처리된 주식 데이터 확인\n",
    "print(\"전처리된 주식 데이터:\")\n",
    "stock_df[['datetime', 'Start', 'End', 'price_change', 'price_change_pct']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처리 (Text Preprocessing)\n",
    "\n",
    "뉴스 텍스트를 분석에 적합한 형태로 전처리합니다. 이 과정에는 불용어 제거, 토큰화 등이 포함됩니다.\n",
    "\n",
    "We preprocess the news text into a form suitable for analysis. This process includes removing stopwords, tokenization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 모듈 임포트\n",
    "from src.data.text_preprocessor import TextPreprocessor\n",
    "\n",
    "# 텍스트 전처리기 초기화\n",
    "preprocessor = TextPreprocessor(\n",
    "    language='korean',\n",
    "    remove_stopwords=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_numbers=False,\n",
    "    use_mecab=True\n",
    ")\n",
    "\n",
    "# 샘플 텍스트 전처리\n",
    "sample_text = news_df['Title'].iloc[0] + \". \" + news_df['Body'].iloc[0][:200]\n",
    "processed_text = preprocessor.preprocess_text(sample_text)\n",
    "\n",
    "print(\"원본 텍스트:\")\n",
    "print(sample_text)\n",
    "print(\"\\n전처리된 텍스트:\")\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 전체 전처리\n",
    "print(\"뉴스 데이터 전처리 중...\")\n",
    "\n",
    "# 제목 전처리\n",
    "news_df['processed_title'] = news_df['Title'].apply(preprocessor.preprocess_text)\n",
    "\n",
    "# 본문 전처리 (시간 절약을 위해 일부만 처리)\n",
    "news_df['processed_body'] = news_df['Body'].apply(\n",
    "    lambda x: preprocessor.preprocess_text(x[:500]) if isinstance(x, str) else \"\"\n",
    ")\n",
    "\n",
    "print(\"전처리 완료!\")\n",
    "\n",
    "# 전처리 결과 확인\n",
    "news_df[['Title', 'processed_title', 'processed_body']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 뉴스 벡터화 (News Vectorization)\n",
    "\n",
    "전처리된 뉴스 텍스트를 벡터로 변환합니다. 여러 벡터화 방법을 비교해 보겠습니다.\n",
    "\n",
    "We convert the preprocessed news text into vectors. Let's compare several vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 벡터화 모듈 임포트\n",
    "from src.models.news_vectorizer import NewsVectorizer\n",
    "\n",
    "# 기본 벡터화 방법 (TF-IDF)\n",
    "tfidf_vectorizer = NewsVectorizer(\n",
    "    method='tfidf',\n",
    "    title_weight=0.7,\n",
    "    content_weight=0.3,\n",
    "    max_features=100\n",
    ")\n",
    "\n",
    "# Word2Vec 벡터화 방법\n",
    "w2v_vectorizer = NewsVectorizer(\n",
    "    method='word2vec',\n",
    "    title_weight=0.7,\n",
    "    content_weight=0.3,\n",
    "    vector_size=100\n",
    ")\n",
    "\n",
    "# 벡터화 수행\n",
    "print(\"TF-IDF 벡터화 수행 중...\")\n",
    "tfidf_vectors = tfidf_vectorizer.vectorize_dataframe(\n",
    "    news_df,\n",
    "    title_col='processed_title',\n",
    "    content_col='processed_body'\n",
    ")\n",
    "\n",
    "print(\"Word2Vec 벡터화 수행 중...\")\n",
    "w2v_vectors = w2v_vectorizer.vectorize_dataframe(\n",
    "    news_df,\n",
    "    title_col='processed_title',\n",
    "    content_col='processed_body'\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF 벡터 형태: {tfidf_vectors.shape}\")\n",
    "print(f\"Word2Vec 벡터 형태: {w2v_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 고급 임베딩 방법 (Advanced Embedding Methods)\n",
    "\n",
    "이제 한국어에 특화된 고급 임베딩 방법을 사용하여 뉴스 텍스트를 벡터화해 보겠습니다.\n",
    "\n",
    "Now, let's vectorize news text using advanced embedding methods specialized for Korean language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 임베딩 향상 모듈 임포트\n",
    "from src.models.korean_embedding_enhancer import KoreanEmbeddingEnhancer\n",
    "\n",
    "# 한국어 임베딩 향상기 초기화\n",
    "embedding_enhancer = KoreanEmbeddingEnhancer(\n",
    "    use_kobert=True,\n",
    "    use_klue_roberta=True,\n",
    "    use_kosimcse=True,\n",
    "    use_bge_korean=True,\n",
    "    cache_embeddings=True\n",
    ")\n",
    "\n",
    "# 사용 가능한 모델 확인\n",
    "available_models = embedding_enhancer.get_available_models()\n",
    "print(f\"사용 가능한 모델: {available_models}\")\n",
    "\n",
    "# 샘플 텍스트에 대한 임베딩 생성\n",
    "sample_texts = news_df['Title'].head(3).tolist()\n",
    "\n",
    "for model in available_models[:2]:  # 처음 두 모델만 시연\n",
    "    print(f\"\\n{model} 모델을 사용한 임베딩:\")\n",
    "    for text in sample_texts:\n",
    "        embedding = embedding_enhancer.get_embedding(text, model)\n",
    "        print(f\"텍스트: {text[:30]}...\")\n",
    "        print(f\"임베딩 차원: {embedding.shape}\")\n",
    "        print(f\"임베딩 샘플: {embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 임베딩 생성\n",
    "print(\"앙상블 임베딩 생성 중...\")\n",
    "ensemble_embeddings = embedding_enhancer.get_ensemble_embeddings(\n",
    "    sample_texts,\n",
    "    models=available_models[:2],  # 처음 두 모델만 사용\n",
    "    weights=[0.6, 0.4]  # 가중치 설정\n",
    ")\n",
    "\n",
    "print(f\"앙상블 임베딩 형태: {ensemble_embeddings.shape}\")\n",
    "print(f\"앙상블 임베딩 샘플:\\n{ensemble_embeddings[0][:5]}...\")\n",
    "\n",
    "# 임베딩 간 유사도 계산\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(ensemble_embeddings)\n",
    "print(\"\\n임베딩 간 유사도 행렬:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 뉴스 클러스터링 (News Clustering)\n",
    "\n",
    "벡터화된 뉴스를 클러스터링하여 유사한 뉴스 그룹을 식별합니다.\n",
    "\n",
    "We cluster the vectorized news to identify groups of similar news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 클러스터링 모듈 임포트\n",
    "from src.models.news_clustering import NewsClustering\n",
    "\n",
    "# 클러스터링 모델 초기화\n",
    "clustering_model = NewsClustering(\n",
    "    method='kmeans',\n",
    "    n_clusters=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 클러스터링 수행\n",
    "print(\"TF-IDF 벡터를 사용한 클러스터링 수행 중...\")\n",
    "tfidf_clusters = clustering_model.cluster(tfidf_vectors)\n",
    "\n",
    "print(\"Word2Vec 벡터를 사용한 클러스터링 수행 중...\")\n",
    "w2v_clusters = clustering_model.cluster(w2v_vectors)\n",
    "\n",
    "# 클러스터링 결과 저장\n",
    "news_df['tfidf_cluster'] = tfidf_clusters\n",
    "news_df['w2v_cluster'] = w2v_clusters\n",
    "\n",
    "# 클러스터링 결과 확인\n",
    "print(\"\\nTF-IDF 클러스터 분포:\")\n",
    "print(news_df['tfidf_cluster'].value_counts())\n",
    "\n",
    "print(\"\\nWord2Vec 클러스터 분포:\")\n",
    "print(news_df['w2v_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 시각화\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA를 사용하여 2차원으로 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_pca = pca.fit_transform(tfidf_vectors)\n",
    "\n",
    "# 클러스터 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster in range(clustering_model.n_clusters):\n",
    "    mask = tfidf_clusters == cluster\n",
    "    plt.scatter(tfidf_pca[mask, 0], tfidf_pca[mask, 1], label=f'Cluster {cluster}', alpha=0.7)\n",
    "\n",
    "plt.title('TF-IDF 벡터를 사용한 뉴스 클러스터링 결과')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 고급 클러스터링 방법 (Advanced Clustering Methods)\n",
    "\n",
    "이제 고급 임베딩을 사용하여 뉴스 클러스터링을 수행해 보겠습니다.\n",
    "\n",
    "Now, let's perform news clustering using advanced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터에 대한 임베딩 생성\n",
    "sample_df = news_df.head(50)  # 처리 시간 단축을 위해 일부만 사용\n",
    "\n",
    "# 데이터프레임 벡터화\n",
    "print(\"KoBERT를 사용한 벡터화 수행 중...\")\n",
    "kobert_vectors = embedding_enhancer.vectorize_dataframe(\n",
    "    sample_df,\n",
    "    text_col='Title',\n",
    "    content_col='Body',\n",
    "    use_content=True,\n",
    "    model='kobert',\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# 앙상블 벡터화\n",
    "print(\"앙상블 벡터화 수행 중...\")\n",
    "ensemble_vectors = embedding_enhancer.vectorize_dataframe_ensemble(\n",
    "    sample_df,\n",
    "    text_col='Title',\n",
    "    content_col='Body',\n",
    "    use_content=True,\n",
    "    models=available_models[:2],\n",
    "    weights=[0.6, 0.4],\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "print(f\"KoBERT 벡터 형태: {kobert_vectors.shape}\")\n",
    "print(f\"앙상블 벡터 형태: {ensemble_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 클러스터링 수행\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# 계층적 클러스터링 모델 초기화\n",
    "hierarchical_clustering = AgglomerativeClustering(\n",
    "    n_clusters=5,\n",
    "    affinity='cosine',\n",
    "    linkage='average'\n",
    ")\n",
    "\n",
    "# 클러스터링 수행\n",
    "kobert_clusters = hierarchical_clustering.fit_predict(kobert_vectors)\n",
    "ensemble_clusters = hierarchical_clustering.fit_predict(ensemble_vectors)\n",
    "\n",
    "# 클러스터링 결과 저장\n",
    "sample_df['kobert_cluster'] = kobert_clusters\n",
    "sample_df['ensemble_cluster'] = ensemble_clusters\n",
    "\n",
    "# 클러스터링 결과 확인\n",
    "print(\"\\nKoBERT 클러스터 분포:\")\n",
    "print(sample_df['kobert_cluster'].value_counts())\n",
    "\n",
    "print(\"\\n앙상블 클러스터 분포:\")\n",
    "print(sample_df['ensemble_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 시각화\n",
    "# PCA를 사용하여 2차원으로 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "ensemble_pca = pca.fit_transform(ensemble_vectors)\n",
    "\n",
    "# 클러스터 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster in range(5):\n",
    "    mask = ensemble_clusters == cluster\n",
    "    plt.scatter(ensemble_pca[mask, 0], ensemble_pca[mask, 1], label=f'Cluster {cluster}', alpha=0.7)\n",
    "\n",
    "plt.title('앙상블 임베딩을 사용한 뉴스 클러스터링 결과')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 주식 영향 분석 (Stock Impact Analysis)\n",
    "\n",
    "뉴스가 주식 가격에 미치는 영향을 분석합니다.\n",
    "\n",
    "We analyze the impact of news on stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주식 영향 분석 모듈 임포트\n",
    "from src.models.stock_impact_analyzer import StockImpactAnalyzer\n",
    "\n",
    "# 주식 영향 분석기 초기화\n",
    "impact_analyzer = StockImpactAnalyzer(\n",
    "    lookback_window=3,  # 3일 전부터의 데이터 사용\n",
    "    lookahead_window=3,  # 3일 후까지의 데이터 사용\n",
    "    impact_threshold=0.5  # 영향력 임계값\n",
    ")\n",
    "\n",
    "# 주식 영향 분석 수행\n",
    "print(\"주식 영향 분석 수행 중...\")\n",
    "impact_scores = impact_analyzer.analyze_impact(\n",
    "    news_df,\n",
    "    stock_df,\n",
    "    news_date_col='datetime',\n",
    "    stock_date_col='datetime',\n",
    "    price_col='End'\n",
    ")\n",
    "\n",
    "# 영향 점수 저장\n",
    "news_df['impact_score'] = impact_scores\n",
    "\n",
    "# 영향 점수 분포 확인\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(news_df['impact_score'].dropna(), bins=20, alpha=0.7)\n",
    "plt.title('뉴스 영향 점수 분포')\n",
    "plt.xlabel('영향 점수')\n",
    "plt.ylabel('뉴스 수')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 영향력이 큰 뉴스 확인\n",
    "high_impact_news = news_df.sort_values('impact_score', ascending=False).head(5)\n",
    "print(\"\\n영향력이 큰 뉴스 Top 5:\")\n",
    "for idx, row in high_impact_news.iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"날짜: {row['Date']}\")\n",
    "    print(f\"영향 점수: {row['impact_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 고급 주식 영향 분석 (Advanced Stock Impact Analysis)\n",
    "\n",
    "이제 다양한 임베딩 모델을 사용한 고급 주식 영향 분석 방법을 시연합니다.\n",
    "\n",
    "Now, we demonstrate advanced stock impact analysis methods using various embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 점수 계산 방법 모듈 임포트\n",
    "from src.models.advanced_scoring_methods import AdvancedScoringMethods\n",
    "\n",
    "# 고급 점수 계산 방법 초기화\n",
    "scoring_methods = AdvancedScoringMethods(\n",
    "    use_kosimcse=True,\n",
    "    use_bge_korean=True,\n",
    "    cache_embeddings=True\n",
    ")\n",
    "\n",
    "# 감성 점수 계산 (간단한 방법으로 대체)\n",
    "def calculate_simple_sentiment(text):\n",
    "    positive_words = ['상승', '급등', '호조', '성장', '개선', '흑자', '최대', '신기록']\n",
    "    negative_words = ['하락', '급락', '부진', '감소', '악화', '적자', '손실', '하향']\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "        \n",
    "    pos_count = sum(1 for word in positive_words if word in text)\n",
    "    neg_count = sum(1 for word in negative_words if word in text)\n",
    "    \n",
    "    if pos_count + neg_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (pos_count - neg_count) / (pos_count + neg_count)\n",
    "\n",
    "# 샘플 데이터에 감성 점수 추가\n",
    "sample_df['sentiment_score'] = sample_df['Title'].apply(calculate_simple_sentiment)\n",
    "\n",
    "# 주가 변동 계산 (간단한 방법으로 대체)\n",
    "sample_df['price_change_pct_1d'] = 0.5  # 예시 값\n",
    "\n",
    "# 앙상블 영향 점수 계산\n",
    "print(\"앙상블 영향 점수 계산 중...\")\n",
    "impact_results = scoring_methods.calculate_ensemble_impact_scores(\n",
    "    sample_df,\n",
    "    text_col='Title',\n",
    "    sentiment_col='sentiment_score',\n",
    "    price_change_col='price_change_pct_1d',\n",
    "    models=['kosimcse', 'bge_korean'],\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\n앙상블 영향 점수 결과:\")\n",
    "impact_results[['Title', 'sentiment_score', 'impact_kosimcse', 'impact_bge_korean', 'impact_ensemble']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 비교 시각화\n",
    "scoring_methods.visualize_model_comparison(\n",
    "    impact_results,\n",
    "    models=['kosimcse', 'bge_korean']\n",
    ")\n",
    "\n",
    "# 앙상블 가중치 시각화\n",
    "scoring_methods.visualize_ensemble_weights(\n",
    "    weights=[0.6, 0.4],\n",
    "    models=['kosimcse', 'bge_korean']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 한국어 텍스트 분석 (Korean Text Analysis)\n",
    "\n",
    "한국어 금융 텍스트에 대한 고급 감성 분석을 수행합니다.\n",
    "\n",
    "We perform advanced sentiment analysis on Korean financial text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 텍스트 분석 모듈 임포트\n",
    "from src.models.korean_text_analyzer import KoreanFinancialTextAnalyzer\n",
    "\n",
    "# 한국어 텍스트 분석기 초기화\n",
    "text_analyzer = KoreanFinancialTextAnalyzer(\n",
    "    use_finbert=True,\n",
    "    use_advanced_embeddings=True,\n",
    "    cache_results=True\n",
    ")\n",
    "\n",
    "# 샘플 텍스트 분석\n",
    "sample_texts = [\n",
    "    \"삼성전자 주가 상승, 실적 개선 기대감에 투자자들 관심 집중\",\n",
    "    \"삼성전자 주가 하락, 실적 부진 우려에 투자자들 매도세 확대\",\n",
    "    \"삼성전자, 신제품 출시 계획 발표. 시장 반응은 중립적\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    result = text_analyzer.analyze_text(text)\n",
    "    print(f\"텍스트: {text}\")\n",
    "    print(f\"감성 점수: {result['sentiment_score']:.4f} ({result['sentiment_label']})\")\n",
    "    print(f\"신뢰도: {result['confidence']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 분석\n",
    "batch_results = text_analyzer.analyze_batch(sample_texts)\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(batch_results)), [r['sentiment_score'] for r in batch_results], alpha=0.7)\n",
    "plt.xticks(range(len(batch_results)), [f\"Text {i+1}\" for i in range(len(batch_results))])\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.title('텍스트 감성 점수')\n",
    "plt.ylabel('감성 점수 (-1 ~ 1)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 뉴스 추천 (News Recommendation)\n",
    "\n",
    "사용자에게 관련성 높은 뉴스를 추천합니다.\n",
    "\n",
    "We recommend relevant news to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 추천 모듈 임포트\n",
    "from src.models.news_recommender import NewsRecommender\n",
    "\n",
    "# 뉴스 추천기 초기화\n",
    "recommender = NewsRecommender(\n",
    "    cf_weight=0.3,  # 협업 필터링 가중치\n",
    "    cbf_weight=0.3,  # 콘텐츠 기반 필터링 가중치\n",
    "    si_weight=0.2,   # 소셜 영향력 가중치\n",
    "    latest_weight=0.1, # 최신성 가중치\n",
    "    stock_impact_weight=0.1 # 주식 영향력 가중치\n",
    ")\n",
    "\n",
    "# 추천 수행\n",
    "print(\"뉴스 추천 수행 중...\")\n",
    "recommended_news = recommender.recommend(\n",
    "    news_df,\n",
    "    user_id=None,  # 비개인화 추천\n",
    "    vectors=tfidf_vectors,\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "# 추천 결과 확인\n",
    "print(\"\\n추천 뉴스:\")\n",
    "for idx, row in recommended_news.iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"날짜: {row['Date']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 고급 뉴스 추천 (Advanced News Recommendation)\n",
    "\n",
    "이제 고급 임베딩과 다양한 점수 계산 방법을 사용한 뉴스 추천을 시연합니다.\n",
    "\n",
    "Now, we demonstrate news recommendation using advanced embeddings and various scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 임베딩을 사용한 추천\n",
    "# 샘플 데이터에 대한 앙상블 임베딩 사용\n",
    "print(\"고급 임베딩을 사용한 추천 수행 중...\")\n",
    "advanced_recommended_news = recommender.recommend(\n",
    "    sample_df,\n",
    "    user_id=None,  # 비개인화 추천\n",
    "    vectors=ensemble_vectors,\n",
    "    top_n=3,\n",
    "    stock_impact_col='impact_ensemble'  # 앙상블 영향 점수 사용\n",
    ")\n",
    "\n",
    "# 추천 결과 확인\n",
    "print(\"\\n고급 임베딩을 사용한 추천 뉴스:\")\n",
    "for idx, row in advanced_recommended_news.iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"날짜: {row['datetime']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 가중치 조합 실험\n",
    "weight_combinations = [\n",
    "    {'cf': 0.2, 'cbf': 0.2, 'si': 0.2, 'latest': 0.2, 'stock_impact': 0.2},  # 균등 가중치\n",
    "    {'cf': 0.1, 'cbf': 0.1, 'si': 0.1, 'latest': 0.1, 'stock_impact': 0.6},  # 주식 영향력 중심\n",
    "    {'cf': 0.1, 'cbf': 0.6, 'si': 0.1, 'latest': 0.1, 'stock_impact': 0.1},  # 콘텐츠 중심\n",
    "    {'cf': 0.1, 'cbf': 0.1, 'si': 0.1, 'latest': 0.6, 'stock_impact': 0.1}   # 최신성 중심\n",
    "]\n",
    "\n",
    "# 각 가중치 조합에 대한 추천 결과 비교\n",
    "for i, weights in enumerate(weight_combinations):\n",
    "    # 추천기 초기화\n",
    "    custom_recommender = NewsRecommender(\n",
    "        cf_weight=weights['cf'],\n",
    "        cbf_weight=weights['cbf'],\n",
    "        si_weight=weights['si'],\n",
    "        latest_weight=weights['latest'],\n",
    "        stock_impact_weight=weights['stock_impact']\n",
    "    )\n",
    "    \n",
    "    # 추천 수행\n",
    "    recommended = custom_recommender.recommend(\n",
    "        sample_df,\n",
    "        user_id=None,\n",
    "        vectors=ensemble_vectors,\n",
    "        top_n=1,\n",
    "        stock_impact_col='impact_ensemble'\n",
    "    )\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"\\n가중치 조합 {i+1}:\")\n",
    "    print(f\"CF: {weights['cf']}, CBF: {weights['cbf']}, SI: {weights['si']}, Latest: {weights['latest']}, Stock Impact: {weights['stock_impact']}\")\n",
    "    print(f\"추천 뉴스: {recommended['Title'].iloc[0]}\")\n",
    "    print(f\"추천 점수: {recommended['recommendation_score'].iloc[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 실험적 모델 (Experimental Models)\n",
    "\n",
    "다양한 실험적 모델과 기법을 시연합니다.\n",
    "\n",
    "We demonstrate various experimental models and techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 임베딩 모델 비교\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 샘플 텍스트\n",
    "comparison_texts = [\n",
    "    \"삼성전자 신제품 출시, 시장 반응 긍정적\",\n",
    "    \"삼성전자 신형 스마트폰 공개, 소비자들 호평\",\n",
    "    \"애플 아이폰 신모델 발표, 삼성전자 경쟁 심화\",\n",
    "    \"반도체 시장 침체, 삼성전자 실적 우려\"\n",
    "]\n",
    "\n",
    "# 각 모델별 임베딩 생성\n",
    "model_embeddings = {}\n",
    "available_models = embedding_enhancer.get_available_models()\n",
    "\n",
    "for model in available_models[:3]:  # 처음 세 모델만 비교\n",
    "    print(f\"{model} 모델 임베딩 생성 중...\")\n",
    "    embeddings = [embedding_enhancer.get_embedding(text, model) for text in comparison_texts]\n",
    "    model_embeddings[model] = np.array(embeddings)\n",
    "\n",
    "# 앙상블 임베딩 생성\n",
    "print(\"앙상블 임베딩 생성 중...\")\n",
    "ensemble_embeddings = embedding_enhancer.get_ensemble_embeddings(\n",
    "    comparison_texts,\n",
    "    models=available_models[:3],\n",
    "    weights=[0.4, 0.3, 0.3]\n",
    ")\n",
    "model_embeddings['ensemble'] = ensemble_embeddings\n",
    "\n",
    "# 각 모델별 유사도 행렬 계산 및 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model, embeddings) in enumerate(model_embeddings.items()):\n",
    "    # 유사도 행렬 계산\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # 히트맵 시각화\n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, ax=axes[i])\n",
    "    axes[i].set_title(f'{model} 모델 유사도 행렬')\n",
    "    axes[i].set_xticklabels([f'Text {i+1}' for i in range(len(comparison_texts))], rotation=45)\n",
    "    axes[i].set_yticklabels([f'Text {i+1}' for i in range(len(comparison_texts))], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 클러스터링 알고리즘 비교\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 임베딩 데이터 준비 (앙상블 임베딩 사용)\n",
    "X = ensemble_embeddings\n",
    "\n",
    "# 클러스터링 알고리즘 정의\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=2),\n",
    "    'SpectralClustering': SpectralClustering(n_clusters=3, random_state=42)\n",
    "}\n",
    "\n",
    "# 각 알고리즘 실행 및 결과 비교\n",
    "results = {}\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    print(f\"{name} 알고리즘 실행 중...\")\n",
    "    labels = algorithm.fit_predict(X)\n",
    "    results[name] = labels\n",
    "    \n",
    "    # 실루엣 점수 계산 (DBSCAN은 클러스터 수가 가변적이므로 제외)\n",
    "    if name != 'DBSCAN':\n",
    "        score = silhouette_score(X, labels)\n",
    "        print(f\"{name} 실루엣 점수: {score:.4f}\")\n",
    "    \n",
    "    # 클러스터 분포 출력\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"클러스터 수: {len(unique_labels)}\")\n",
    "    for label in unique_labels:\n",
    "        print(f\"클러스터 {label}: {np.sum(labels == label)}개 샘플\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 시스템 통합 (System Integration)\n",
    "\n",
    "전체 시스템을 통합하여 완전한 파이프라인을 시연합니다.\n",
    "\n",
    "We integrate the entire system to demonstrate a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 시스템 통합 파이프라인\n",
    "def run_clear_pipeline(news_data, stock_data, use_advanced_models=False):\n",
    "    print(\"CLEAR 시스템 파이프라인 실행 중...\")\n",
    "    \n",
    "    # 1. 데이터 전처리\n",
    "    print(\"1. 데이터 전처리 중...\")\n",
    "    # 날짜 형식 변환\n",
    "    news_data['datetime'] = pd.to_datetime(news_data['Date'].apply(convert_date_format))\n",
    "    news_data = news_data.sort_values('datetime')\n",
    "    \n",
    "    stock_data['datetime'] = pd.to_datetime(stock_data['Date'] + ' ' + stock_data['Time'])\n",
    "    stock_data = stock_data.sort_values('datetime')\n",
    "    \n",
    "    # 주식 가격 변동 계산\n",
    "    stock_data['price_change'] = stock_data['End'] - stock_data['Start']\n",
    "    stock_data['price_change_pct'] = (stock_data['price_change'] / stock_data['Start']) * 100\n",
    "    \n",
    "    # 2. 텍스트 전처리\n",
    "    print(\"2. 텍스트 전처리 중...\")\n",
    "    preprocessor = TextPreprocessor(language='korean', remove_stopwords=True)\n",
    "    news_data['processed_title'] = news_data['Title'].apply(preprocessor.preprocess_text)\n",
    "    news_data['processed_body'] = news_data['Body'].apply(\n",
    "        lambda x: preprocessor.preprocess_text(x[:500]) if isinstance(x, str) else \"\"\n",
    "    )\n",
    "    \n",
    "    # 3. 뉴스 벡터화\n",
    "    print(\"3. 뉴스 벡터화 중...\")\n",
    "    if use_advanced_models:\n",
    "        # 고급 임베딩 사용\n",
    "        embedding_enhancer = KoreanEmbeddingEnhancer(\n",
    "            use_kobert=True,\n",
    "            use_klue_roberta=True\n",
    "        )\n",
    "        vectors = embedding_enhancer.vectorize_dataframe_ensemble(\n",
    "            news_data.head(50),  # 시연을 위해 일부만 사용\n",
    "            text_col='Title',\n",
    "            content_col='Body',\n",
    "            use_content=True,\n",
    "            models=['kobert', 'klue_roberta'],\n",
    "            weights=[0.6, 0.4],\n",
    "            batch_size=10\n",
    "        )\n",
    "        news_subset = news_data.head(50)\n",
    "    else:\n",
    "        # 기본 벡터화 방법 사용\n",
    "        vectorizer = NewsVectorizer(method='tfidf', title_weight=0.7, content_weight=0.3)\n",
    "        vectors = vectorizer.vectorize_dataframe(\n",
    "            news_data,\n",
    "            title_col='processed_title',\n",
    "            content_col='processed_body'\n",
    "        )\n",
    "        news_subset = news_data\n",
    "    \n",
    "    # 4. 뉴스 클러스터링\n",
    "    print(\"4. 뉴스 클러스터링 중...\")\n",
    "    clustering_model = NewsClustering(method='kmeans', n_clusters=5)\n",
    "    clusters = clustering_model.cluster(vectors)\n",
    "    news_subset['cluster'] = clusters\n",
    "    \n",
    "    # 5. 주식 영향 분석\n",
    "    print(\"5. 주식 영향 분석 중...\")\n",
    "    if use_advanced_models:\n",
    "        # 고급 점수 계산 방법 사용\n",
    "        # 감성 점수 계산 (간단한 방법으로 대체)\n",
    "        news_subset['sentiment_score'] = news_subset['Title'].apply(calculate_simple_sentiment)\n",
    "        news_subset['price_change_pct_1d'] = 0.5  # 예시 값\n",
    "        \n",
    "        scoring_methods = AdvancedScoringMethods(use_kosimcse=True)\n",
    "        impact_results = scoring_methods.calculate_ensemble_impact_scores(\n",
    "            news_subset,\n",
    "            text_col='Title',\n",
    "            sentiment_col='sentiment_score',\n",
    "            price_change_col='price_change_pct_1d'\n",
    "        )\n",
    "        news_subset = impact_results\n",
    "        impact_col = 'impact_ensemble'\n",
    "    else:\n",
    "        # 기본 영향 분석 방법 사용\n",
    "        impact_analyzer = StockImpactAnalyzer(lookback_window=3, lookahead_window=3)\n",
    "        impact_scores = impact_analyzer.analyze_impact(\n",
    "            news_subset,\n",
    "            stock_data,\n",
    "            news_date_col='datetime',\n",
    "            stock_date_col='datetime',\n",
    "            price_col='End'\n",
    "        )\n",
    "        news_subset['impact_score'] = impact_scores\n",
    "        impact_col = 'impact_score'\n",
    "    \n",
    "    # 6. 뉴스 추천\n",
    "    print(\"6. 뉴스 추천 중...\")\n",
    "    recommender = NewsRecommender(\n",
    "        cf_weight=0.2,\n",
    "        cbf_weight=0.3,\n",
    "        si_weight=0.2,\n",
    "        latest_weight=0.1,\n",
    "        stock_impact_weight=0.2\n",
    "    )\n",
    "    recommended_news = recommender.recommend(\n",
    "        news_subset,\n",
    "        user_id=None,  # 비개인화 추천\n",
    "        vectors=vectors,\n",
    "        top_n=5,\n",
    "        stock_impact_col=impact_col\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCLEAR 시스템 파이프라인 실행 완료!\")\n",
    "    return recommended_news\n",
    "\n",
    "# 기본 모델로 파이프라인 실행\n",
    "print(\"기본 모델로 파이프라인 실행:\")\n",
    "basic_recommendations = run_clear_pipeline(news_df.copy(), stock_df.copy(), use_advanced_models=False)\n",
    "\n",
    "print(\"\\n기본 모델 추천 결과:\")\n",
    "for idx, row in basic_recommendations.head(3).iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 모델로 파이프라인 실행\n",
    "print(\"고급 모델로 파이프라인 실행:\")\n",
    "advanced_recommendations = run_clear_pipeline(news_df.copy(), stock_df.copy(), use_advanced_models=True)\n",
    "\n",
    "print(\"\\n고급 모델 추천 결과:\")\n",
    "for idx, row in advanced_recommendations.head(3).iterrows():\n",
    "    print(f\"제목: {row['Title']}\")\n",
    "    print(f\"추천 점수: {row['recommendation_score']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론 (Conclusion)\n",
    "\n",
    "이 노트북에서는 NAVER의 AiRS 알고리즘을 기반으로 한 뉴스 추천 및 주식 영향 분석 시스템인 CLEAR의 기능을 시연했습니다. 시스템은 다음과 같은 주요 구성 요소로 이루어져 있습니다:\n",
    "\n",
    "1. 데이터 처리 (뉴스 및 주식 데이터)\n",
    "2. 텍스트 전처리\n",
    "3. 뉴스 벡터화 (기본 및 고급 임베딩 방법)\n",
    "4. 뉴스 클러스터링\n",
    "5. 주식 영향 분석 (기본 및 고급 점수 계산 방법)\n",
    "6. 뉴스 추천\n",
    "\n",
    "또한 다양한 한국어 임베딩 모델과 점수 계산 방법을 비교하고, 이를 통합하여 완전한 파이프라인을 구현했습니다.\n",
    "\n",
    "In this notebook, we demonstrated the functionality of CLEAR, a news recommendation and stock impact analysis system based on NAVER's AiRS algorithm. The system consists of the following main components:\n",
    "\n",
    "1. Data processing (news and stock data)\n",
    "2. Text preprocessing\n",
    "3. News vectorization (basic and advanced embedding methods)\n",
    "4. News clustering\n",
    "5. Stock impact analysis (basic and advanced scoring methods)\n",
    "6. News recommendation\n",
    "\n",
    "We also compared various Korean embedding models and scoring methods, and integrated them to implement a complete pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
